{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSVm2q6pXdlWq1rL0KQyvj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VrijKun/CS6910_Assignement_3/blob/main/DL_Assignement_3_local_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lHT4qEMQnyv",
        "outputId": "20f8d442-c9e5-45d5-b70d-30bc56a72709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 14 21:52:21 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 551.78                 Driver Version: 551.78         CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA T1000                 WDDM  |   00000000:01:00.0  On |                  N/A |\n",
            "| 35%   46C    P8             N/A /   50W |     643MiB /   4096MiB |      1%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A      2120    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
            "|    0   N/A  N/A      6832    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
            "|    0   N/A  N/A     11084    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
            "|    0   N/A  N/A     11332    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
            "|    0   N/A  N/A     11372    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
            "|    0   N/A  N/A     11384    C+G   ...__8wekyb3d8bbwe\\Microsoft.Notes.exe      N/A      |\n",
            "|    0   N/A  N/A     14204    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
            "|    0   N/A  N/A     15380    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
            "|    0   N/A  N/A     15760    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
            "|    0   N/A  N/A     17796    C+G   ...e Stream\\90.0.3.0\\GoogleDriveFS.exe      N/A      |\n",
            "|    0   N/A  N/A     19784    C+G   ...on\\124.0.2478.97\\msedgewebview2.exe      N/A      |\n",
            "|    0   N/A  N/A     20332    C+G   ...\\Local\\slack\\app-4.38.115\\slack.exe      N/A      |\n",
            "|    0   N/A  N/A     21420    C+G   ...on\\124.0.2478.80\\msedgewebview2.exe      N/A      |\n",
            "|    0   N/A  N/A     22132    C+G   ...41.0_x64__v10z8vjag6ke6\\HP.myHP.exe      N/A      |\n",
            "|    0   N/A  N/A     22436    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
            "|    0   N/A  N/A     23052    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
            "|    0   N/A  N/A     27348    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
            "|    0   N/A  N/A     28404    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For local run time you don't have to run this\n",
        "!yes | wget \"https://drive.google.com/drive/folders/1tmm5HT2Zwj-4vDZzRFc4Av4KoWHYowoG?usp=sharing\""
      ],
      "metadata": {
        "id": "DNTkPBjP9dlM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a43a698-241a-4053-8b8b-1a14a5c3c735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'yes' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For local run time you don't have to run this\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "6vIU8i0W7h5V",
        "outputId": "f48765ae-c066-4d72-b83f-635e5e0207c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# For local run time you don't have to run this\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIpsTgHh9eYR",
        "outputId": "58de07a5-4f53-4ada-cd39-f308ddfd54a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas\n",
            "  Downloading pandas-2.0.3-cp38-cp38-win_amd64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from pandas) (2023.3.post1)\n",
            "Collecting tzdata>=2022.1 (from pandas)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from pandas) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading pandas-2.0.3-cp38-cp38-win_amd64.whl (10.8 MB)\n",
            "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.1/10.8 MB 2.1 MB/s eta 0:00:05\n",
            "    --------------------------------------- 0.2/10.8 MB 1.5 MB/s eta 0:00:08\n",
            "    --------------------------------------- 0.2/10.8 MB 1.3 MB/s eta 0:00:09\n",
            "   - -------------------------------------- 0.3/10.8 MB 1.4 MB/s eta 0:00:08\n",
            "   - -------------------------------------- 0.4/10.8 MB 1.3 MB/s eta 0:00:09\n",
            "   - -------------------------------------- 0.4/10.8 MB 1.3 MB/s eta 0:00:08\n",
            "   - -------------------------------------- 0.5/10.8 MB 1.2 MB/s eta 0:00:09\n",
            "   - -------------------------------------- 0.5/10.8 MB 1.3 MB/s eta 0:00:08\n",
            "   -- ------------------------------------- 0.6/10.8 MB 1.2 MB/s eta 0:00:09\n",
            "   -- ------------------------------------- 0.6/10.8 MB 1.2 MB/s eta 0:00:09\n",
            "   -- ------------------------------------- 0.6/10.8 MB 1.1 MB/s eta 0:00:09\n",
            "   -- ------------------------------------- 0.7/10.8 MB 1.1 MB/s eta 0:00:10\n",
            "   -- ------------------------------------- 0.7/10.8 MB 1.1 MB/s eta 0:00:10\n",
            "   -- ------------------------------------- 0.7/10.8 MB 975.9 kB/s eta 0:00:11\n",
            "   -- ------------------------------------- 0.7/10.8 MB 969.4 kB/s eta 0:00:11\n",
            "   -- ------------------------------------- 0.7/10.8 MB 969.4 kB/s eta 0:00:11\n",
            "   -- ------------------------------------- 0.7/10.8 MB 873.6 kB/s eta 0:00:12\n",
            "   -- ------------------------------------- 0.8/10.8 MB 847.7 kB/s eta 0:00:12\n",
            "   -- ------------------------------------- 0.8/10.8 MB 841.5 kB/s eta 0:00:12\n",
            "   --- ------------------------------------ 0.8/10.8 MB 845.4 kB/s eta 0:00:12\n",
            "   --- ------------------------------------ 0.8/10.8 MB 814.4 kB/s eta 0:00:13\n",
            "   --- ------------------------------------ 0.9/10.8 MB 795.6 kB/s eta 0:00:13\n",
            "   --- ------------------------------------ 0.9/10.8 MB 795.6 kB/s eta 0:00:13\n",
            "   --- ------------------------------------ 0.9/10.8 MB 795.6 kB/s eta 0:00:13\n",
            "   --- ------------------------------------ 0.9/10.8 MB 795.6 kB/s eta 0:00:13\n",
            "   --- ------------------------------------ 0.9/10.8 MB 694.8 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 694.8 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 678.1 kB/s eta 0:00:15\n",
            "   --- ------------------------------------ 0.9/10.8 MB 142.7 kB/s eta 0:01:09\n",
            "   --- ------------------------------------ 1.0/10.8 MB 144.8 kB/s eta 0:01:08\n",
            "   --- ------------------------------------ 1.0/10.8 MB 147.2 kB/s eta 0:01:07\n",
            "   --- ------------------------------------ 1.0/10.8 MB 147.3 kB/s eta 0:01:07\n",
            "   --- ------------------------------------ 1.0/10.8 MB 150.9 kB/s eta 0:01:05\n",
            "   --- ------------------------------------ 1.0/10.8 MB 150.9 kB/s eta 0:01:05\n",
            "   --- ------------------------------------ 1.0/10.8 MB 152.2 kB/s eta 0:01:04\n",
            "   --- ------------------------------------ 1.1/10.8 MB 154.5 kB/s eta 0:01:03\n",
            "   --- ------------------------------------ 1.1/10.8 MB 154.5 kB/s eta 0:01:03\n",
            "   ---- ----------------------------------- 1.1/10.8 MB 156.8 kB/s eta 0:01:02\n",
            "   ---- ----------------------------------- 1.1/10.8 MB 156.8 kB/s eta 0:01:02\n",
            "   ---- ----------------------------------- 1.1/10.8 MB 156.8 kB/s eta 0:01:02\n",
            "   ---- ----------------------------------- 1.1/10.8 MB 156.8 kB/s eta 0:01:02\n",
            "   ---- ----------------------------------- 1.1/10.8 MB 156.0 kB/s eta 0:01:02\n",
            "   ---- ----------------------------------- 1.2/10.8 MB 160.3 kB/s eta 0:01:00\n",
            "   ---- ----------------------------------- 1.2/10.8 MB 166.3 kB/s eta 0:00:58\n",
            "   ---- ----------------------------------- 1.2/10.8 MB 166.3 kB/s eta 0:00:58\n",
            "   ---- ----------------------------------- 1.2/10.8 MB 166.3 kB/s eta 0:00:58\n",
            "   ---- ----------------------------------- 1.2/10.8 MB 166.3 kB/s eta 0:00:58\n",
            "   ---- ----------------------------------- 1.2/10.8 MB 166.3 kB/s eta 0:00:58\n",
            "   ---- ----------------------------------- 1.2/10.8 MB 166.3 kB/s eta 0:00:58\n",
            "   ---- ----------------------------------- 1.2/10.8 MB 166.3 kB/s eta 0:00:58\n",
            "   ---- ----------------------------------- 1.2/10.8 MB 166.3 kB/s eta 0:00:58\n",
            "   ---- ----------------------------------- 1.2/10.8 MB 166.3 kB/s eta 0:00:58\n",
            "   ---- ----------------------------------- 1.2/10.8 MB 159.2 kB/s eta 0:01:00\n",
            "   ---- ----------------------------------- 1.3/10.8 MB 162.5 kB/s eta 0:00:59\n",
            "   ---- ----------------------------------- 1.3/10.8 MB 165.8 kB/s eta 0:00:58\n",
            "   ----- ---------------------------------- 1.4/10.8 MB 175.9 kB/s eta 0:00:54\n",
            "   ----- ---------------------------------- 1.4/10.8 MB 180.4 kB/s eta 0:00:52\n",
            "   ----- ---------------------------------- 1.5/10.8 MB 190.6 kB/s eta 0:00:49\n",
            "   ----- ---------------------------------- 1.6/10.8 MB 198.8 kB/s eta 0:00:47\n",
            "   ------ --------------------------------- 1.7/10.8 MB 207.4 kB/s eta 0:00:44\n",
            "   ------ --------------------------------- 1.7/10.8 MB 213.4 kB/s eta 0:00:43\n",
            "   ------ --------------------------------- 1.8/10.8 MB 218.9 kB/s eta 0:00:42\n",
            "   ------ --------------------------------- 1.8/10.8 MB 219.7 kB/s eta 0:00:41\n",
            "   ------ --------------------------------- 1.8/10.8 MB 219.7 kB/s eta 0:00:41\n",
            "   ------ --------------------------------- 1.9/10.8 MB 222.2 kB/s eta 0:00:41\n",
            "   ------- -------------------------------- 1.9/10.8 MB 230.3 kB/s eta 0:00:39\n",
            "   ------- -------------------------------- 2.0/10.8 MB 233.5 kB/s eta 0:00:38\n",
            "   ------- -------------------------------- 2.1/10.8 MB 245.9 kB/s eta 0:00:36\n",
            "   -------- ------------------------------- 2.2/10.8 MB 254.2 kB/s eta 0:00:34\n",
            "   -------- ------------------------------- 2.3/10.8 MB 262.4 kB/s eta 0:00:33\n",
            "   -------- ------------------------------- 2.3/10.8 MB 266.2 kB/s eta 0:00:32\n",
            "   -------- ------------------------------- 2.4/10.8 MB 278.0 kB/s eta 0:00:31\n",
            "   --------- ------------------------------ 2.4/10.8 MB 281.0 kB/s eta 0:00:30\n",
            "   --------- ------------------------------ 2.5/10.8 MB 286.1 kB/s eta 0:00:29\n",
            "   --------- ------------------------------ 2.5/10.8 MB 286.9 kB/s eta 0:00:29\n",
            "   --------- ------------------------------ 2.6/10.8 MB 289.4 kB/s eta 0:00:29\n",
            "   --------- ------------------------------ 2.6/10.8 MB 290.8 kB/s eta 0:00:29\n",
            "   --------- ------------------------------ 2.6/10.8 MB 292.0 kB/s eta 0:00:28\n",
            "   --------- ------------------------------ 2.6/10.8 MB 292.9 kB/s eta 0:00:28\n",
            "   --------- ------------------------------ 2.7/10.8 MB 294.8 kB/s eta 0:00:28\n",
            "   ---------- ----------------------------- 2.7/10.8 MB 298.4 kB/s eta 0:00:27\n",
            "   ---------- ----------------------------- 2.7/10.8 MB 298.4 kB/s eta 0:00:27\n",
            "   ---------- ----------------------------- 2.8/10.8 MB 299.7 kB/s eta 0:00:27\n",
            "   ---------- ----------------------------- 2.8/10.8 MB 301.5 kB/s eta 0:00:27\n",
            "   ---------- ----------------------------- 2.8/10.8 MB 304.4 kB/s eta 0:00:27\n",
            "   ---------- ----------------------------- 2.8/10.8 MB 304.5 kB/s eta 0:00:27\n",
            "   ---------- ----------------------------- 2.9/10.8 MB 307.5 kB/s eta 0:00:26\n",
            "   ---------- ----------------------------- 2.9/10.8 MB 309.1 kB/s eta 0:00:26\n",
            "   ---------- ----------------------------- 2.9/10.8 MB 310.9 kB/s eta 0:00:26\n",
            "   ----------- ---------------------------- 3.0/10.8 MB 314.2 kB/s eta 0:00:25\n",
            "   ----------- ---------------------------- 3.0/10.8 MB 314.8 kB/s eta 0:00:25\n",
            "   ----------- ---------------------------- 3.0/10.8 MB 314.4 kB/s eta 0:00:25\n",
            "   ----------- ---------------------------- 3.0/10.8 MB 314.4 kB/s eta 0:00:25\n",
            "   ----------- ---------------------------- 3.1/10.8 MB 314.0 kB/s eta 0:00:25\n",
            "   ----------- ---------------------------- 3.1/10.8 MB 316.7 kB/s eta 0:00:25\n",
            "   ----------- ---------------------------- 3.2/10.8 MB 320.4 kB/s eta 0:00:24\n",
            "   ----------- ---------------------------- 3.2/10.8 MB 323.0 kB/s eta 0:00:24\n",
            "   ----------- ---------------------------- 3.2/10.8 MB 323.0 kB/s eta 0:00:24\n",
            "   ------------ --------------------------- 3.2/10.8 MB 324.6 kB/s eta 0:00:24\n",
            "   ------------ --------------------------- 3.2/10.8 MB 324.6 kB/s eta 0:00:24\n",
            "   ------------ --------------------------- 3.3/10.8 MB 324.6 kB/s eta 0:00:24\n",
            "   ------------ --------------------------- 3.3/10.8 MB 327.7 kB/s eta 0:00:23\n",
            "   ------------ --------------------------- 3.3/10.8 MB 328.2 kB/s eta 0:00:23\n",
            "   ------------ --------------------------- 3.4/10.8 MB 327.7 kB/s eta 0:00:23\n",
            "   ------------ --------------------------- 3.4/10.8 MB 330.7 kB/s eta 0:00:23\n",
            "   ------------ --------------------------- 3.4/10.8 MB 332.7 kB/s eta 0:00:23\n",
            "   ------------ --------------------------- 3.4/10.8 MB 332.7 kB/s eta 0:00:23\n",
            "   ------------ --------------------------- 3.4/10.8 MB 332.7 kB/s eta 0:00:23\n",
            "   ------------ --------------------------- 3.4/10.8 MB 332.7 kB/s eta 0:00:23\n",
            "   ------------- -------------------------- 3.6/10.8 MB 339.4 kB/s eta 0:00:22\n",
            "   ------------- -------------------------- 3.6/10.8 MB 340.3 kB/s eta 0:00:22\n",
            "   ------------- -------------------------- 3.7/10.8 MB 346.0 kB/s eta 0:00:21\n",
            "   ------------- -------------------------- 3.7/10.8 MB 350.3 kB/s eta 0:00:21\n",
            "   ------------- -------------------------- 3.8/10.8 MB 351.2 kB/s eta 0:00:20\n",
            "   -------------- ------------------------- 3.8/10.8 MB 356.3 kB/s eta 0:00:20\n",
            "   -------------- ------------------------- 3.9/10.8 MB 359.4 kB/s eta 0:00:20\n",
            "   -------------- ------------------------- 4.0/10.8 MB 363.6 kB/s eta 0:00:19\n",
            "   -------------- ------------------------- 4.0/10.8 MB 364.4 kB/s eta 0:00:19\n",
            "   -------------- ------------------------- 4.0/10.8 MB 367.4 kB/s eta 0:00:19\n",
            "   --------------- ------------------------ 4.1/10.8 MB 373.3 kB/s eta 0:00:18\n",
            "   --------------- ------------------------ 4.2/10.8 MB 376.3 kB/s eta 0:00:18\n",
            "   --------------- ------------------------ 4.2/10.8 MB 379.8 kB/s eta 0:00:18\n",
            "   --------------- ------------------------ 4.2/10.8 MB 381.1 kB/s eta 0:00:18\n",
            "   --------------- ------------------------ 4.3/10.8 MB 384.0 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.3/10.8 MB 385.6 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.4/10.8 MB 388.5 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 391.8 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 393.0 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 393.0 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 393.0 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 393.0 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 393.0 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 393.0 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 393.0 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 393.0 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 393.0 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 393.0 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 393.0 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 377.2 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 377.2 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 377.2 kB/s eta 0:00:17\n",
            "   ---------------- ----------------------- 4.5/10.8 MB 377.2 kB/s eta 0:00:17\n",
            "   ----------------- ---------------------- 4.6/10.8 MB 379.2 kB/s eta 0:00:17\n",
            "   ----------------- ---------------------- 4.6/10.8 MB 379.9 kB/s eta 0:00:17\n",
            "   ----------------- ---------------------- 4.7/10.8 MB 381.3 kB/s eta 0:00:17\n",
            "   ----------------- ---------------------- 4.7/10.8 MB 383.2 kB/s eta 0:00:16\n",
            "   ----------------- ---------------------- 4.8/10.8 MB 386.7 kB/s eta 0:00:16\n",
            "   ------------------ --------------------- 4.9/10.8 MB 391.4 kB/s eta 0:00:16\n",
            "   ------------------ --------------------- 4.9/10.8 MB 394.5 kB/s eta 0:00:15\n",
            "   ------------------ --------------------- 4.9/10.8 MB 394.5 kB/s eta 0:00:15\n",
            "   ------------------ --------------------- 4.9/10.8 MB 394.0 kB/s eta 0:00:15\n",
            "   ------------------ --------------------- 5.0/10.8 MB 397.0 kB/s eta 0:00:15\n",
            "   ------------------ --------------------- 5.0/10.8 MB 398.8 kB/s eta 0:00:15\n",
            "   ------------------ --------------------- 5.1/10.8 MB 401.6 kB/s eta 0:00:15\n",
            "   ------------------- -------------------- 5.2/10.8 MB 404.7 kB/s eta 0:00:14\n",
            "   ------------------- -------------------- 5.2/10.8 MB 406.1 kB/s eta 0:00:14\n",
            "   ------------------- -------------------- 5.3/10.8 MB 410.0 kB/s eta 0:00:14\n",
            "   ------------------- -------------------- 5.3/10.8 MB 412.5 kB/s eta 0:00:14\n",
            "   ------------------- -------------------- 5.3/10.8 MB 413.9 kB/s eta 0:00:14\n",
            "   ------------------- -------------------- 5.4/10.8 MB 414.8 kB/s eta 0:00:14\n",
            "   -------------------- ------------------- 5.4/10.8 MB 414.8 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.4/10.8 MB 415.7 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.4/10.8 MB 415.7 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.5/10.8 MB 414.6 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.5/10.8 MB 415.4 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.5/10.8 MB 415.4 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.5/10.8 MB 415.4 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.5/10.8 MB 415.4 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.5/10.8 MB 415.4 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.5/10.8 MB 415.4 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.5/10.8 MB 415.4 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.5/10.8 MB 415.4 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.5/10.8 MB 415.4 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.5/10.8 MB 415.4 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.5/10.8 MB 415.4 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.5/10.8 MB 415.4 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.5/10.8 MB 415.4 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.5/10.8 MB 397.7 kB/s eta 0:00:14\n",
            "   -------------------- ------------------- 5.5/10.8 MB 398.2 kB/s eta 0:00:14\n",
            "   -------------------- ------------------- 5.6/10.8 MB 398.7 kB/s eta 0:00:14\n",
            "   -------------------- ------------------- 5.6/10.8 MB 400.1 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.6/10.8 MB 399.9 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.6/10.8 MB 400.1 kB/s eta 0:00:13\n",
            "   -------------------- ------------------- 5.7/10.8 MB 399.9 kB/s eta 0:00:13\n",
            "   --------------------- ------------------ 5.7/10.8 MB 401.0 kB/s eta 0:00:13\n",
            "   --------------------- ------------------ 5.7/10.8 MB 401.0 kB/s eta 0:00:13\n",
            "   --------------------- ------------------ 5.7/10.8 MB 400.8 kB/s eta 0:00:13\n",
            "   --------------------- ------------------ 5.8/10.8 MB 401.4 kB/s eta 0:00:13\n",
            "   --------------------- ------------------ 5.8/10.8 MB 401.1 kB/s eta 0:00:13\n",
            "   --------------------- ------------------ 5.8/10.8 MB 401.1 kB/s eta 0:00:13\n",
            "   --------------------- ------------------ 5.8/10.8 MB 400.0 kB/s eta 0:00:13\n",
            "   --------------------- ------------------ 5.8/10.8 MB 400.0 kB/s eta 0:00:13\n",
            "   --------------------- ------------------ 5.8/10.8 MB 400.0 kB/s eta 0:00:13\n",
            "   --------------------- ------------------ 5.8/10.8 MB 398.3 kB/s eta 0:00:13\n",
            "   --------------------- ------------------ 5.9/10.8 MB 399.4 kB/s eta 0:00:13\n",
            "   --------------------- ------------------ 5.9/10.8 MB 399.8 kB/s eta 0:00:13\n",
            "   --------------------- ------------------ 5.9/10.8 MB 399.2 kB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 5.9/10.8 MB 400.3 kB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 5.9/10.8 MB 400.3 kB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 5.9/10.8 MB 399.3 kB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 6.0/10.8 MB 399.4 kB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 6.0/10.8 MB 399.4 kB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 6.0/10.8 MB 399.4 kB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 6.0/10.8 MB 399.4 kB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 6.0/10.8 MB 399.4 kB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 6.0/10.8 MB 395.0 kB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 6.1/10.8 MB 395.8 kB/s eta 0:00:12\n",
            "   ---------------------- ----------------- 6.1/10.8 MB 395.6 kB/s eta 0:00:12\n",
            "   ---------------------- ----------------- 6.1/10.8 MB 395.6 kB/s eta 0:00:12\n",
            "   ---------------------- ----------------- 6.1/10.8 MB 395.6 kB/s eta 0:00:12\n",
            "   ---------------------- ----------------- 6.1/10.8 MB 395.6 kB/s eta 0:00:12\n",
            "   ---------------------- ----------------- 6.1/10.8 MB 395.6 kB/s eta 0:00:12\n",
            "   ---------------------- ----------------- 6.1/10.8 MB 395.6 kB/s eta 0:00:12\n",
            "   ---------------------- ----------------- 6.1/10.8 MB 395.6 kB/s eta 0:00:12\n",
            "   ---------------------- ----------------- 6.1/10.8 MB 395.6 kB/s eta 0:00:12\n",
            "   ---------------------- ----------------- 6.1/10.8 MB 386.7 kB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 6.1/10.8 MB 386.7 kB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 6.1/10.8 MB 386.3 kB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 6.1/10.8 MB 387.1 kB/s eta 0:00:12\n",
            "   ---------------------- ----------------- 6.2/10.8 MB 389.0 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.2/10.8 MB 389.0 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.2/10.8 MB 389.0 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.2/10.8 MB 387.3 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.3/10.8 MB 388.8 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.3/10.8 MB 390.4 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.3/10.8 MB 391.1 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.3/10.8 MB 391.1 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.3/10.8 MB 391.1 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.3/10.8 MB 391.1 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.3/10.8 MB 391.1 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.3/10.8 MB 391.1 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.3/10.8 MB 391.1 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.3/10.8 MB 383.4 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.4/10.8 MB 383.0 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.4/10.8 MB 383.0 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.4/10.8 MB 383.9 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.4/10.8 MB 383.9 kB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 6.4/10.8 MB 383.9 kB/s eta 0:00:12\n",
            "   ------------------------ --------------- 6.5/10.8 MB 384.7 kB/s eta 0:00:12\n",
            "   ------------------------ --------------- 6.5/10.8 MB 384.7 kB/s eta 0:00:12\n",
            "   ------------------------ --------------- 6.5/10.8 MB 384.7 kB/s eta 0:00:12\n",
            "   ------------------------ --------------- 6.5/10.8 MB 384.7 kB/s eta 0:00:12\n",
            "   ------------------------ --------------- 6.5/10.8 MB 384.7 kB/s eta 0:00:12\n",
            "   ------------------------ --------------- 6.5/10.8 MB 384.7 kB/s eta 0:00:12\n",
            "   ------------------------ --------------- 6.5/10.8 MB 384.7 kB/s eta 0:00:12\n",
            "   ------------------------ --------------- 6.5/10.8 MB 384.7 kB/s eta 0:00:12\n",
            "   ------------------------ --------------- 6.5/10.8 MB 384.7 kB/s eta 0:00:12\n",
            "   ------------------------ --------------- 6.5/10.8 MB 376.1 kB/s eta 0:00:12\n",
            "   ------------------------ --------------- 6.5/10.8 MB 376.3 kB/s eta 0:00:12\n",
            "   ------------------------ --------------- 6.6/10.8 MB 377.3 kB/s eta 0:00:12\n",
            "   ------------------------ --------------- 6.6/10.8 MB 377.1 kB/s eta 0:00:12\n",
            "   ------------------------ --------------- 6.6/10.8 MB 380.0 kB/s eta 0:00:11\n",
            "   ------------------------ --------------- 6.7/10.8 MB 381.2 kB/s eta 0:00:11\n",
            "   ------------------------ --------------- 6.7/10.8 MB 381.2 kB/s eta 0:00:11\n",
            "   ------------------------ --------------- 6.7/10.8 MB 381.2 kB/s eta 0:00:11\n",
            "   ------------------------ --------------- 6.7/10.8 MB 381.2 kB/s eta 0:00:11\n",
            "   ------------------------ --------------- 6.7/10.8 MB 381.2 kB/s eta 0:00:11\n",
            "   ------------------------ --------------- 6.7/10.8 MB 381.2 kB/s eta 0:00:11\n",
            "   ------------------------ --------------- 6.7/10.8 MB 381.2 kB/s eta 0:00:11\n",
            "   ------------------------ --------------- 6.7/10.8 MB 381.2 kB/s eta 0:00:11\n",
            "   ------------------------ --------------- 6.7/10.8 MB 381.2 kB/s eta 0:00:11\n",
            "   ------------------------ --------------- 6.7/10.8 MB 381.2 kB/s eta 0:00:11\n",
            "   ------------------------- -------------- 6.7/10.8 MB 373.4 kB/s eta 0:00:11\n",
            "   ------------------------- -------------- 6.8/10.8 MB 375.9 kB/s eta 0:00:11\n",
            "   ------------------------- -------------- 6.9/10.8 MB 378.9 kB/s eta 0:00:11\n",
            "   ------------------------- -------------- 6.9/10.8 MB 379.8 kB/s eta 0:00:11\n",
            "   ------------------------- -------------- 6.9/10.8 MB 380.6 kB/s eta 0:00:11\n",
            "   ------------------------- -------------- 7.0/10.8 MB 382.9 kB/s eta 0:00:10\n",
            "   -------------------------- ------------- 7.0/10.8 MB 384.2 kB/s eta 0:00:10\n",
            "   -------------------------- ------------- 7.1/10.8 MB 387.1 kB/s eta 0:00:10\n",
            "   -------------------------- ------------- 7.2/10.8 MB 389.8 kB/s eta 0:00:10\n",
            "   --------------------------- ------------ 7.3/10.8 MB 393.9 kB/s eta 0:00:09\n",
            "   --------------------------- ------------ 7.3/10.8 MB 395.7 kB/s eta 0:00:09\n",
            "   --------------------------- ------------ 7.4/10.8 MB 399.1 kB/s eta 0:00:09\n",
            "   --------------------------- ------------ 7.5/10.8 MB 401.9 kB/s eta 0:00:09\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.3 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.9 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.9 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.9 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.9 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.9 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.9 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.9 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.9 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.9 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.9 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.9 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.9 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.9 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.9 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.6/10.8 MB 405.9 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.7/10.8 MB 397.8 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.7/10.8 MB 397.8 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.8/10.8 MB 396.4 kB/s eta 0:00:08\n",
            "   ---------------------------- ----------- 7.8/10.8 MB 396.5 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 7.8/10.8 MB 397.5 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 7.9/10.8 MB 398.4 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 7.9/10.8 MB 398.4 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 7.9/10.8 MB 398.5 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 7.9/10.8 MB 399.4 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 7.9/10.8 MB 399.4 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 7.9/10.8 MB 399.4 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 7.9/10.8 MB 399.4 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 7.9/10.8 MB 399.4 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 7.9/10.8 MB 399.4 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 7.9/10.8 MB 399.4 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 7.9/10.8 MB 399.4 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 7.9/10.8 MB 399.4 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 7.9/10.8 MB 399.4 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 7.9/10.8 MB 390.4 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 8.0/10.8 MB 390.9 kB/s eta 0:00:08\n",
            "   ----------------------------- ---------- 8.0/10.8 MB 393.3 kB/s eta 0:00:07\n",
            "   ------------------------------ --------- 8.1/10.8 MB 396.1 kB/s eta 0:00:07\n",
            "   ------------------------------ --------- 8.2/10.8 MB 398.9 kB/s eta 0:00:07\n",
            "   ------------------------------ --------- 8.3/10.8 MB 401.3 kB/s eta 0:00:07\n",
            "   ------------------------------- -------- 8.4/10.8 MB 405.3 kB/s eta 0:00:06\n",
            "   ------------------------------- -------- 8.5/10.8 MB 409.7 kB/s eta 0:00:06\n",
            "   ------------------------------- -------- 8.5/10.8 MB 411.7 kB/s eta 0:00:06\n",
            "   ------------------------------- -------- 8.6/10.8 MB 413.7 kB/s eta 0:00:06\n",
            "   -------------------------------- ------- 8.6/10.8 MB 415.1 kB/s eta 0:00:06\n",
            "   -------------------------------- ------- 8.7/10.8 MB 417.4 kB/s eta 0:00:05\n",
            "   -------------------------------- ------- 8.7/10.8 MB 417.4 kB/s eta 0:00:05\n",
            "   -------------------------------- ------- 8.7/10.8 MB 417.4 kB/s eta 0:00:05\n",
            "   -------------------------------- ------- 8.7/10.8 MB 417.4 kB/s eta 0:00:05\n",
            "   -------------------------------- ------- 8.7/10.8 MB 417.4 kB/s eta 0:00:05\n",
            "   -------------------------------- ------- 8.7/10.8 MB 417.4 kB/s eta 0:00:05\n",
            "   -------------------------------- ------- 8.7/10.8 MB 417.4 kB/s eta 0:00:05\n",
            "   -------------------------------- ------- 8.7/10.8 MB 417.4 kB/s eta 0:00:05\n",
            "   -------------------------------- ------- 8.7/10.8 MB 417.4 kB/s eta 0:00:05\n",
            "   -------------------------------- ------- 8.7/10.8 MB 417.4 kB/s eta 0:00:05\n",
            "   -------------------------------- ------- 8.7/10.8 MB 408.2 kB/s eta 0:00:06\n",
            "   -------------------------------- ------- 8.8/10.8 MB 408.4 kB/s eta 0:00:05\n",
            "   -------------------------------- ------- 8.8/10.8 MB 408.4 kB/s eta 0:00:05\n",
            "   -------------------------------- ------- 8.9/10.8 MB 412.3 kB/s eta 0:00:05\n",
            "   --------------------------------- ------ 9.0/10.8 MB 416.4 kB/s eta 0:00:05\n",
            "   --------------------------------- ------ 9.1/10.8 MB 419.7 kB/s eta 0:00:05\n",
            "   ---------------------------------- ----- 9.2/10.8 MB 425.0 kB/s eta 0:00:04\n",
            "   ---------------------------------- ----- 9.3/10.8 MB 427.8 kB/s eta 0:00:04\n",
            "   ----------------------------------- ---- 9.5/10.8 MB 434.4 kB/s eta 0:00:04\n",
            "   ----------------------------------- ---- 9.5/10.8 MB 436.8 kB/s eta 0:00:03\n",
            "   ----------------------------------- ---- 9.7/10.8 MB 442.4 kB/s eta 0:00:03\n",
            "   ------------------------------------ --- 9.7/10.8 MB 444.2 kB/s eta 0:00:03\n",
            "   ------------------------------------ --- 9.8/10.8 MB 446.9 kB/s eta 0:00:03\n",
            "   ------------------------------------ --- 10.0/10.8 MB 451.6 kB/s eta 0:00:02\n",
            "   ------------------------------------- -- 10.1/10.8 MB 456.3 kB/s eta 0:00:02\n",
            "   ------------------------------------- -- 10.2/10.8 MB 459.0 kB/s eta 0:00:02\n",
            "   ------------------------------------- -- 10.2/10.8 MB 459.0 kB/s eta 0:00:02\n",
            "   -------------------------------------- - 10.3/10.8 MB 461.7 kB/s eta 0:00:02\n",
            "   -------------------------------------- - 10.4/10.8 MB 461.7 kB/s eta 0:00:01\n",
            "   ---------------------------------------  10.6/10.8 MB 464.0 kB/s eta 0:00:01\n",
            "   ---------------------------------------  10.7/10.8 MB 465.0 kB/s eta 0:00:01\n",
            "   ---------------------------------------- 10.8/10.8 MB 465.1 kB/s eta 0:00:00\n",
            "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "   ---------------------------------------- 0.0/345.4 kB ? eta -:--:--\n",
            "   --------- ------------------------------ 81.9/345.4 kB 4.5 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 174.1/345.4 kB 2.1 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 266.2/345.4 kB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 345.4/345.4 kB 1.9 MB/s eta 0:00:00\n",
            "Installing collected packages: tzdata, pandas\n",
            "Successfully installed pandas-2.0.3 tzdata-2024.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pds\n",
        "\n",
        "# function for loading data\n",
        "def dt_ld(path):\n",
        "  with open(path) as fil :\n",
        "    dt = pds.read_csv(fil,sep=',',header=None,names=[\"eng\",\"hin\",\"\"],skip_blank_lines=True,index_col=None)\n",
        "  word_data = dt[dt['eng'].notna()]\n",
        "  word_data = dt[dt['hin'].notna()]\n",
        "  word_data = dt[['eng', 'hin']]\n",
        "  return word_data\n",
        "\n",
        "\n",
        "# Define the file paths for train, valid, and test data on your local machine\n",
        "train_file_path = r\"C:\\Users\\ASL 5\\Downloads\\aksharantar_sampled\\hin\\hin_train.csv\"\n",
        "valid_file_path = r\"C:\\Users\\ASL 5\\Downloads\\aksharantar_sampled\\hin\\hin_valid.csv\"\n",
        "test_file_path = r\"C:\\Users\\ASL 5\\Downloads\\aksharantar_sampled\\hin\\hin_test.csv\"\n",
        "\n",
        "# Load the data using the defined function\n",
        "train_data = dt_ld(train_file_path)\n",
        "valid_data = dt_ld(valid_file_path)\n",
        "test_data = dt_ld(test_file_path)\n",
        "\n",
        "# Display test_data\n",
        "print(test_data)\n",
        "\n",
        "# Extracting 'eng' and 'hin' columns into lists\n",
        "test_hin = list(test_data['hin'])\n",
        "test_eng = list(test_data['eng'])\n",
        "\n",
        "# Some visualization of data\n",
        "print(test_eng)\n",
        "print(test_hin)\n",
        "print(len(train_data))\n",
        "print(len(valid_data))\n",
        "print(len(test_data))"
      ],
      "metadata": {
        "id": "1d_5GXyE-dLF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a600b37-6d75-4333-abe0-caf8077e6e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'charmap' codec can't decode byte 0x8d in position 20: character maps to <undefined>",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m test_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mASL 5\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124maksharantar_sampled\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhin\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhin_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Load the data using the defined function\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mdt_ld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m valid_data \u001b[38;5;241m=\u001b[39m dt_ld(valid_file_path)\n\u001b[0;32m     21\u001b[0m test_data \u001b[38;5;241m=\u001b[39m dt_ld(test_file_path)\n",
            "Cell \u001b[1;32mIn[20], line 6\u001b[0m, in \u001b[0;36mdt_ld\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdt_ld\u001b[39m(path):\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path) \u001b[38;5;28;01mas\u001b[39;00m fil :\n\u001b[1;32m----> 6\u001b[0m     dt \u001b[38;5;241m=\u001b[39m \u001b[43mpds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfil\u001b[49m\u001b[43m,\u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mskip_blank_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m   word_data \u001b[38;5;241m=\u001b[39m dt[dt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna()]\n\u001b[0;32m      8\u001b[0m   word_data \u001b[38;5;241m=\u001b[39m dt[dt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhin\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna()]\n",
            "File \u001b[1;32m~\\.conda\\envs\\Cuda121_pytorch222\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\.conda\\envs\\Cuda121_pytorch222\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32m~\\.conda\\envs\\Cuda121_pytorch222\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\.conda\\envs\\Cuda121_pytorch222\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32m~\\.conda\\envs\\Cuda121_pytorch222\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
            "File \u001b[1;32m~\\.conda\\envs\\Cuda121_pytorch222\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:550\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m~\\.conda\\envs\\Cuda121_pytorch222\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:742\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m~\\.conda\\envs\\Cuda121_pytorch222\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m~\\.conda\\envs\\Cuda121_pytorch222\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m~\\.conda\\envs\\Cuda121_pytorch222\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:2021\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8d in position 20: character maps to <undefined>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# You'll need to change the path based on the plce you have saved the files on your directory\n",
        "train_data = dt_ld(r\"C:\\Users\\ASL 5\\Downloads\\aksharantar_sampled\\hin\\hin_train.csv\")\n",
        "valid_data = dt_ld(r\"C:\\Users\\ASL 5\\Downloads\\aksharantar_sampled\\hin\\hin_valid.csv\")\n",
        "test_data = dt_ld(r\"C:\\Users\\ASL 5\\Downloads\\aksharantar_sampled\\hin\\hin_test.csv\")\n",
        "\n",
        "\n",
        "print(test_data)\n",
        "# Saving the data in CSV's again in list form\n",
        "test_hin = list(test_data['hin'])\n",
        "test_eng = list(test_data['eng'])\n",
        "\n",
        "# Some visualization of data\n",
        "print(test_eng)\n",
        "print(test_hin)\n",
        "print(len(train_data))\n",
        "print(len(valid_data))\n",
        "print(len(test_data))\n",
        "#print(\"test_hindi: \", test_hin)\n",
        "#print(\"test_english: \", test_eng)\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "train_data = dt_ld(\"/content/drive/MyDrive/aksharantar_sampled/hin/hin_train.csv\")\n",
        "valid_data = dt_ld(\"/content/drive/MyDrive/aksharantar_sampled/hin/hin_valid.csv\")\n",
        "test_data = dt_ld(\"/content/drive/MyDrive/aksharantar_sampled/hin/hin_test.csv\")\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "3QWSRb3k9TRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the function for loading data\n",
        "def dt_ld(path):\n",
        "    # Load data from the CSV file using pandas\n",
        "    dt = pd.read_csv(path, sep=',', header=None, names=[\"eng\", \"hin\", \"\"], skip_blank_lines=True, index_col=None, encoding='utf-8')\n",
        "\n",
        "    # Filter out rows where either 'eng' or 'hin' column is NaN\n",
        "    word_data = dt.dropna(subset=['eng', 'hin'])\n",
        "\n",
        "    # Select only 'eng' and 'hin' columns\n",
        "    word_data = word_data[['eng', 'hin']]\n",
        "\n",
        "    return word_data\n",
        "\n",
        "# Define the file paths for train, valid, and test data on your local machine\n",
        "train_file_path = r\"C:\\Users\\ASL 5\\Downloads\\aksharantar_sampled\\hin\\hin_train.csv\"\n",
        "valid_file_path = r\"C:\\Users\\ASL 5\\Downloads\\aksharantar_sampled\\hin\\hin_valid.csv\"\n",
        "test_file_path = r\"C:\\Users\\ASL 5\\Downloads\\aksharantar_sampled\\hin\\hin_test.csv\"\n",
        "\n",
        "# Load the data using the defined function\n",
        "train_data = dt_ld(train_file_path)\n",
        "valid_data = dt_ld(valid_file_path)\n",
        "test_data = dt_ld(test_file_path)\n",
        "\n",
        "# Display test_data\n",
        "print(test_data)\n",
        "\n",
        "# Extracting 'eng' and 'hin' columns into lists\n",
        "test_hin = list(test_data['hin'])\n",
        "test_eng = list(test_data['eng'])\n",
        "\n",
        "# Some visualization of data\n",
        "print(test_eng)\n",
        "print(test_hin)\n",
        "print(len(train_data))\n",
        "print(len(valid_data))\n",
        "print(len(test_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM37DYXfEbLZ",
        "outputId": "0dc8e573-d4c6-4a25-dc3f-d7d7dd4d572f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               eng          hin\n",
            "0          thermax      \n",
            "1        sikhaaega      \n",
            "2            learn         \n",
            "3         twitters     \n",
            "4      tirunelveli  \n",
            "...            ...          ...\n",
            "4091       saflata       \n",
            "4092        shbana        \n",
            "4093  khaatootolaa     \n",
            "4094    shivastava     \n",
            "4095  preranapuree  \n",
            "\n",
            "[4096 rows x 2 columns]\n",
            "['thermax', 'sikhaaega', 'learn', 'twitters', 'tirunelveli', 'independence', 'speshiyon', 'shurooh', 'kolhapur', 'ajhar', 'karaar', 'anka', 'wpd', 'haashie', 'glendale', 'udhed', 'ekthi', 'idea', 'ambikapur', 'makerere', 'saboodaane', 'foohadta', 'sequent', 'shueb', 'panihati', 'sametati', 'ukhrul', 'brahmlin', 'utaraadhikaaree', 'iqbal', 'dayaalapuraa', 'sohrai', 'takreeban', 'farrukhnagar', 'theinga', 'tyoiharon', 'karneshvardhaam', 'umanath', 'daanshil', 'saahityotsav', 'shantiniketan', 'shikayatkarta', 'andarkhane', 'panter', 'leedaron', 'galgand', 'kaarniyaan', 'murgipaalan', 'mushahid', 'modules', 'rajouri', 'sushrushaa', 'shringaar', 'holt', 'laigikata', 'ijaajat', 'vankshetra', 'bhutal', 'swaadpremiyon', 'nineteez', 'frektar', 'likhkar', 'eyarkandeeshnar', 'nabz', 'quess', 'bouni', 'kaaragujaariyaan', 'gaangnam', 'tapia', 'tezpur', 'talve', 'seemaai', 'darshnaarthi', 'rivas', 'tarkvaad', 'anusaarakaa', 'coachella', 'latakakar', 'patravaliyan', 'parishad', 'spinj', 'anshida', 'dejesus', 'saraaymohiuddinpur', 'lowell', 'capacitor', 'passengerjind', 'granthiyon', 'buena', 'canterbury', 'kaathiyavadi', 'tekchandani', 'fisad', 'beraharamee', 'nishkarshah', 'activities', 'rikailleebreshan', 'shasanadhikaariyon', 'fijoolkharchi', 'dablyoopeedee', 'pace', 'dastar', 'catlin', 'joddta', 'killat', 'gruhnagar', 'wonder', 'vatar', 'shving', 'pashtun', 'farm', 'dibanu', 'pashchamee', 'uthapatak', 'nilaabh', 'laxmeeniyaan', 'mahaanagarawaasiyon', 'upneta', 'convention', 'sharp', 'kaaryayojana', 'maas', 'westing', 'kurvetee', 'jyaada', 'mukeshvari', 'shrimati', 'vivekadhikaron', 'loksabhavalee', 'kabahaa', 'bhraantiyon', 'vivekahin', 'balmiki', 'haryana', 'jivraj', 'flynn', 'rana', 'vishnupura', 'ghotaalebaajon', 'hairatangenz', 'takaleephadeh', 'peepaadh', 'dhabhaashaaraa', 'antdiyan', 'robin', 'singar', 'gumshudagi', 'balkrishna', 'phabti', 'palatne', 'lahlahaati', 'nagaada', 'udanen', 'klein', 'juloos', 'karyabhar', 'manto', 'paimaish', 'covina', 'penshan', 'ogastaavestalaind', 'centreeng', 'activa', 'barker', 'valkan', 'vruton', 'kabaddi', 'raunakh', 'caitlyn', 'phoos', 'jangadnaa', 'upakhyaanon', 'sundram', 'cochin', 'neko', 'kaushal', 'phaliya', 'acevedo', 'kottayam', 'dilaaega', 'kurvi', 'saunpengi', 'lekhan', 'gudepu', 'pharase', 'nubiya', 'taja', 'aantrit', 'qazi', 'murgipalan', 'eddy', 'nagine', 'subha', 'luis', 'vijayapuram', 'bataao', 'ochoa', 'science', 'sarkarein', 'bremerton', 'gurupado', 'sanyukt', 'raalod', 'baadhaon', 'gaumutra', 'sabhaen', 'tani', 'delhi', 'suken', 'vineet', 'chimate', 'kikiyana', 'rocket', 'maxwell', 'hippo', 'deepan', 'arymaa', 'bhulata', 'karmaacharee', 'durgapur', 'tribhaashaa', 'soodkhoron', 'prastaavon', 'survin', 'prapti', 'girivaasiyon', 'siducing', 'welwet', 'muqarrar', 'baxter', 'denier', 'cw', 'uthani', 'ghasi', 'ikattha', 'bhupsingh', 'dabochne', 'crosby', 'naameegiraamee', 'hurley', 'jarda', 'liepradhanmantri', 'bhranti', 'agencyyon', 'bago', 'forcee', 'bosaan', 'karki', 'ginate', 'manokaamnaaon', 'sammpattiyaan', 'politheenon', 'mtake', 'parichar', 'badabadee', 'crane', 'laamiyaan', 'ulhasnagar', 'prabhanmantri', 'shiwalapurwa', 'ferth', 'suhagin', 'saraikela', 'romeoville', 'raajsthanvasiyon', 'pratirodhaatmak', 'svarnikaa', 'vyavasthapana', 'tel', 'galatfahmi', 'takeinsider', 'microfiber', 'shreemahapoorn', 'fallujaa', 'donaldson', 'chadhne', 'jeevvigyaaniyon', 'cricketing', 'reduction', 'awaazen', 'wlaad', 'prathmik', 'thenga', 'excel', 'chatakaate', 'kubuddin', 'taaza', 'padani', 'nichaai', 'wausau', 'aalochakon', 'jaljale', 'surat', 'jeevati', 'prahaarak', 'shreesadguru', 'sanchalanon', 'bhal', 'tughlaq', 'uthaapatak', 'hastaantaraneey', 'utsaahawarddhan', 'sangoshtree', 'shaasanadhikariyon', 'mahadhipatiyon', 'fatima', 'fode', 'kantinyoo', 'kaamanaen', 'chutile', 'sankhyaen', 'kranj', 'barsata', 'parikalpit', 'harpal', 'maaoont', 'grover', 'biolozis', 'halisahar', 'angelo', 'janeudharee', 'bichbachaav', 'pokar', 'apvartit', 'aastin', 'pratishthaavaale', 'buenaventura', 'deccan', 'seriw', 'pardaa', 'emile', 'shaili', 'eastvale', 'chhallaa', 'lakshyon', 'hoaf', 'jarurih', 'rahya', 'mahkane', 'devi', 'nielsen', 'bhira', 'chabutare', 'bhushan', 'rukte', 'aamdani', 'talkshow', 'junagadh', 'padhata', 'odhakar', 'broke', 'talaasha', 'parivaarwaale', 'trubyunal', 'brock', 'yallaappaa', 'tasvaron', 'tugalaki', 'pick', 'circulation', 'jvar', 'kanindham', 'kush', 'properfacebuk', 'ashrafi', 'chadhen', 'vangchuk', 'honeywell', 'radiumdharmee', 'ladhege', 'sachan', 'arthshastriyon', 'canton', 'aatmamugdha', 'pontiac', 'lopez', 'ghontanaa', 'ambani', 'maryadaon', 'ratlam', 'hercules', 'akola', 'sakshan', 'notbandeeshuda', 'lagavaatee', 'alive', 'jivant', 'foda', 'fatahee', 'noobiyaa', 'lalmanee', 'faisala', 'krutyaa', 'dhuan', 'ahuja', 'pratipaalpur', 'namkeen', 'manjilen', 'laffaajee', 'juraab', 'fulae', 'castalloy', 'black', 'sooraj', 'bronch', 'avarnataa', 'johari', 'tevree', 'sanders', 'welder', 'eneeyoo', 'bharashtachaariyon', 'laalmanee', 'khaatedaar', 'aazmaanaa', 'aavrittiyon', 'kullawee', 'bahawalpur', 'prathmikta', 'mobile', 'katihar', 'swecha', 'haamidan', 'samdharshni', 'dridh', 'gond', 'wishesh', 'kaabilegour', 'laurense', 'hisham', 'chausinga', 'pafin', 'mate', 'buildcon', 'teekon', 'athak', 'jindagee', 'chayal', 'maap', 'last', 'gajra', 'ratrichar', 'kaathalaa', 'sahuuliyata', 'pattnaik', 'nyayalayeen', 'mukhyamantriyon', 'jeetanewalon', 'swaphoto', 'clifton', 'doda', 'pendulum', 'chipakane', 'nalakasa', 'chhah', 'gond', 'sameekshaakartaa', 'mantripad', 'liberation', 'asamaany', 'chungiyon', 'arica', 'balot', 'thin', 'formic', 'yuddhak', 'vegas', 'las', 'chitranshi', 'avranta', 'wasting', 'vitark', 'mukabaale', 'dharmapoorwee', 'ambaani', 'ibija', 'mahamedia', 'suzlon', 'varnit', 'jhuthlane', 'madurai', 'kaundal', 'lapatta', 'kshin', 'tewaree', 'raichur', 'lindon', 'pulama', 'olraunder', 'garibon', 'callahan', 'wabag', 'kaipdhari', 'atta', 'bareli', 'jing', 'jivan', 'reddy', 'pehni', 'tanu', 'deepika', 'lohati', 'kakkad', 'maulviyon', 'dungarpur', 'zaalaanaa', 'lakme', 'prospectus', 'westinghouse', 'bawara', 'thakkr', 'ichchhaachaaree', 'triya', 'dhavakon', 'damle', 'shravanabelagola', 'nilkamal', 'karmacharee', 'poorwottar', 'lehrayi', 'haito', 'prakaandh', 'khelmantree', 'bhavishya', 'egmor', 'shringar', 'saupengee', 'knoxville', 'bender', 'sundaram', 'pradesh', 'samvedikaran', 'laguna', 'bhisham', 'yemmiganur', 'huhtamaki', 'vadli', 'gail', 'race', 'salazar', 'diphtheria', 'dradh', 'shadadhi', 'prabodhini', 'lokpratinidhiyon', 'jatati', 'mangla', 'berwyn', 'mum', 'udaygadhi', 'akapulko', 'vaidhruti', 'charanbaddh', 'mandadi', 'euclid', 'paarampariktaa', 'hafate', 'sathiya', 'sanghiyataa', 'mukhyaatithiyon', 'vyavasthavirodhi', 'raktadaataaon', 'baramade', 'tarutal', 'kutta', 'jhaadaa', 'kishun', 'schaeffler', 'amrohi', 'raidical', 'jenner', 'varshiya', 'hasino', 'shaavakon', 'rupa', 'swaarthparak', 'weronika', 'webb', 'mahapatra', 'chenkein', 'agra', 'jiyaangkou', 'westwork', 'janaganna', 'pasco', 'jadhav', 'enterprise', 'sanshayon', 'varnavyavasthaavaadiyon', 'upasthiyon', 'mudichu', 'parnstaron', 'ia', 'baila', 'livaal', 'hillsboro', 'rashtriyadhyaksha', 'peedablyootee', 'thousand', 'shaakir', 'salahkaar', 'vishwakarma', 'ridge', 'swaasthyaheenataa', 'rishvatkhor', 'praavinses', 'daftarvaale', 'physician', 'log', 'jodhika', 'sevalaa', 'doobat', 'sipah', 'bharamaa', 'prtigyaa', 'meghwal', 'nandoi', 'aurora', 'formuley', 'durghtna', 'swatantrasingh', 'monterey', 'baalti', 'columbia', 'whittier', 'dukke', 'cubeck', 'chamakna', 'sheelbhag', 'bistagond', 'findlay', 'bapaa', 'dehra', 'aise', 'shrankhala', 'hastaaksharon', 'tirupati', 'mangalmoortih', 'kursiyon', 'mithaiyaan', 'pahno', 'halebid', 'bangsh', 'officially', 'sevai', 'gurjar', 'samanvayaka', 'parnel', 'sunsan', 'cusex', 'madhya', 'yudhrat', 'urbandale', 'briggs', 'error', 'chikhali', 'waan', 'adhikaariyaan', 'roanoke', 'haokip', 'poorvapekshaaon', 'masheenawat', 'kadmo', 'kurami', 'vidyaathyon', 'simtataa', 'sangrur', 'strongsville', 'agvaai', 'jangan', 'nagraj', 'yariyan', 'bhagyeshwar', 'jivati', 'sudkhoron', 'rukwaakar', 'brahmvarta', 'gray', 'bhraanti', 'purooshottamapur', 'teji', 'svaatan', 'mantarimandaliya', 'ashley', 'mathews', 'moolon', 'akzo', 'hamirpur', 'pokhriyal', 'bolateen', 'pratishthavale', 'dabangata', 'dhuriya', 'teap', 'gazi', 'ijajat', 'mejbani', 'toledo', 'mohadhaa', 'putli', 'sanchari', 'taapanee', 'little', 'narnaari', 'dhaniye', 'gadhoonga', 'faadi', 'gammat', 'tuberculosis', 'nagarjuna', 'raajneechi', 'medizensar', 'laphphaajee', 'commerce', 'maathaloo', 'chatagaon', 'hilig', 'isnpector', 'testing', 'naushad', 'falluja', 'pik', 'jiwan', 'katal', 'dablyooeeef', 'besil', 'slidon', 'saindan', 'jeremy', 'gaanv', 'todenge', 'udaan', 'saptkraanti', 'miranda', 'vyapak', 'maddy', 'sabhasadon', 'nauvahan', 'atmaen', 'rosa', 'damaae', 'midwest', 'trelarah', 'marlborough', 'waco', 'vimarshon', 'khan', 'difarard', 'sarsaa', 'hester', 'uttar', 'kasera', 'kuraaane', 'dayitvon', 'chayaniton', 'morales', 'heros', 'premamoolak', 'moran', 'zydus', 'nelson', 'amarpal', 'phatne', 'tinka', 'gaayikaao', 'pakayen', 'bhrashtachariyon', 'chalit', 'affle', 'mod', 'rajnitikaaron', 'arthon', 'gaaligalauj', 'anyonyashritata', 'udaya', 'cutlery', 'pratibandhah', 'neelaabh', 'bilae', 'khullamkhulla', 'swanson', 'anvaahaarya', 'rewari', 'sahkalakar', 'championship', 'bhadkaayaa', 'namdhari', 'alba', 'watanukoollan', 'khaatadhaarak', 'texas', 'special', 'harvansh', 'dinu', 'shaniwar', 'de', 'dabi', 'nahinkarvaaee', 'maarvaad', 'peediyon', 'brigton', 'mnaey', 'ligar', 'dilaega', 'aadhin', 'parwa', 'hempstead', 'mamooli', 'pradeshikaa', 'verification', 'fefada', 'wisangatiyaa', 'kachchhe', 'ghost', 'pipli', 'lovasa', 'helper', 'doobnewaale', 'bakhshane', 'indriyonke', 'barhad', 'risaalee', 'kaksheevati', 'vishvarakt', 'lodi', 'peachtree', 'ghumakkdon', 'ojha', 'ilaaj', 'arunai', 'ayodhyeanath', 'bhautikwadiyon', 'roket', 'nurochemistry', 'coldvel', 'aawaazen', 'mool', 'multiplexon', 'tyoihar', 'rotation', 'belacha', 'sahakalaakar', 'jadit', 'nagaaraa', 'barpaya', 'doongarpur', 'land', 'uchchhrunkhalataa', 'failaiya', 'suchi', 'noon', 'rp', 'shield', 'palanpur', 'electrosteel', 'obscura', 'cigretton', 'dikhaee', 'russell', 'nazarandaaz', 'kualalampoor', 'austin', 'raidcras', 'tcns', 'svaayattshasan', 'mood', 'daulatabad', 'bhulta', 'pratikaara', 'nights', 'nakam', 'shutting', 'raashtra', 'bechkar', 'stone', 'wardha', 'deedablyutee', 'desabathula', 'baapa', 'baghbaan', 'aadeshah', 'udelkar', 'tarko', 'yaaddaasht', 'inhalation', 'safradganj', 'roopak', 'scranton', 'varnavyavasthaawaadiyon', 'shikshakarmiyon', 'khandelwal', 'beverages', 'pidiya', 'nadaf', 'jehe', 'dhahaaenge', 'mastmaula', 'agartala', 'jamayaa', 'pardesi', 'arya', 'velvet', 'aapdaein', 'aagashe', 'achchi', 'udghosh', 'swabhawatah', 'bani', 'raykar', 'pidilite', 'dual', 'bheera', 'silva', 'adhikaran', 'rangkata', 'srichakra', 'gowmutra', 'nichalaa', 'burhanpur', 'sahooliyata', 'labdh', 'poole', 'strasion', 'picking', 'oditar', 'deniar', 'ajit', 'langdon', 'avashist', 'jha', 'prashasnik', 'bossier', 'badhachadhakar', 'giridih', 'bhuj', 'reading', 'fawaare', 'advait', 'rukate', 'anonditaa', 'fontana', 'nyutriyo', 'pailoton', 'jaden', 'boriwali', 'ilaichi', 'alliance', 'dhammaa', 'nahargadh', 'nsiu', 'sapaat', 'lengdan', 'panama', 'putle', 'aasaapaas', 'alwadhi', 'mufalisi', 'sansathaan', 'darshnaarthee', 'lokotsav', 'graahakonne', 'uchchaadhikaareeyo', 'bhav', 'reja', 'motherson', 'kauplaiks', 'aaburod', 'heidelbergceat', 'chimati', 'surfactants', 'mitao', 'kailana', 'tankar', 'odhkar', 'merta', 'megavaal', 'rasyanik', 'soochnah', 'sugamata', 'jockey', 'munaf', 'niswarth', 'kshetradwaaraa', 'mahawar', 'buldhana', 'dhamkata', 'bhusvami', 'vigyan', 'quete', 'bevkufana', 'jajmau', 'beltran', 'vilas', 'makhdumpur', 'kolorado', 'blackburn', 'suniti', 'ghigghee', 'flavonoids', 'chahunmukhi', 'supachye', 'siratee', 'vishnu', 'chaudahavan', 'chaudhari', 'aabru', 'balurak', 'davangere', 'anurit', 'shipping', 'patiala', 'jaanam', 'financial', 'damae', 'policy', 'icc', 'kuchchh', 'cheekhna', 'reliance', 'urvarakon', 'ravael', 'ladhkiyon', 'mukeshwaree', 'wankshetra', 'ummeede', 'kahaavah', 'kaantipoorn', 'badaaen', 'enseeaardablyusee', 'pratibaddhataa', 'follow', 'sevamukta', 'evanston', 'charas', 'hills', 'robertsan', 'bhadauriya', 'glaxosmithkline', 'producton', 'guevara', 'vigyaanon', 'jarvis', 'laphz', 'font', 'paradheen', 'shak', 'ganjhoo', 'dit', 'danptiyon', 'maadhikhedhaa', 'pratishthaanon', 'soond', 'ginte', 'alagalag', 'swastikakaar', 'pratikaraa', 'aastik', 'niveshkartaa', 'saval', 'sanskritiyan', 'raubdar', 'raghuram', 'intaravyu', 'chharhari', 'sawyer', 'bhasapa', 'jvaaron', 'sansthita', 'parshdon', 'pulatskar', 'uthaaeange', 'laengee', 'oeeef', 'valsad', 'mulon', 'putliyan', 'shmil', 'riggs', 'camarillo', 'saflataa', 'vinasht', 'kping', 'jayalalithaa', 'items', 'soccer', 'asphalt', 'mehrotra', 'chune', 'zarakh', 'mejbaani', 'muflisi', 'luna', 'warnavyavasthaavaadiyon', 'lipaaee', 'uthni', 'bajaj', 'hisarah', 'dampatiyon', 'watson', 'augustustine', 'pataakshep', 'dhandhekhoree', 'ubhregaa', 'anton', 'apardarshi', 'nihlaani', 'tivolee', 'hogis', 'winter', 'badalavaakar', 'ghumaee', 'petaluma', 'beeemdablyooem', 'bakhshate', 'sanvedikaran', 'telon', 'ghummakkadon', 'haanfte', 'farheen', 'james', 'kiratpur', 'redding', 'shaq', 'surprise', 'kisanon', 'chalenge', 'yoder', 'stein', 'paithelojist', 'labeling', 'ikthiyosis', 'liberation', 'phoodon', 'nanhein', 'bhadauria', 'jatilaa', 'pahunchanee', 'raashtron', 'blossom', 'tandon', 'gadbadbadee', 'sanasthita', 'yatraen', 'aadan', 'javate', 'niyuttiyon', 'bhishm', 'hundiya', 'murgeepaalan', 'malekar', 'loveland', 'mavey', 'vikram', 'hanfte', 'barrackpore', 'ghaziabad', 'tarkon', 'spf', 'artemis', 'aipp', 'jaiban', 'gairairaadatan', 'vivekadhin', 'sabajooniyar', 'faliya', 'prekshakganon', 'purvaasiyon', 'vipannata', 'maanga', 'ordanens', 'pradhan', 'abhiyuktonke', 'viratkhand', 'hijbul', 'vidyaon', 'manju', 'chhawindra', 'chhupkar', 'doranda', 'saym', 'mithhaiyaan', 'jeevvigyaniyon', 'pashtuun', 'brahraachaarinee', 'jerusalem', 'altamonte', 'sihuntaa', 'hemwati', 'koshadhikaaree', 'ashanka', 'bean', 'vaigyanikata', 'jheelon', 'robarto', 'shrirampur', 'manchiya', 'vasoolataa', 'morrison', 'mhagranth', 'transistor', 'sarvjaatiya', 'patotsav', 'houkee', 'medithsan', 'udana', 'saraaymohiuddeenpur', 'petersburg', 'ranjkatha', 'diya', 'mohindar', 'mastek', 'nirbadh', 'ehsaas', 'avrutiyon', 'eshiyavan', 'thki', 'turbine', 'dimaak', 'jodhpur', 'durdhara', 'khaastaur', 'makrani', 'high', 'fatahi', 'mariya', 'ganv', 'pendulam', 'aavishakaar', 'natijeh', 'jeffersonville', 'brahmasiddhi', 'tulshidas', 'reeta', 'charchaamanch', 'pratikaaraa', 'tarkwaad', 'dabangon', 'hichkicha', 'sheershakon', 'dam', 'dviarthee', 'jawabah', 'bahumaan', 'jaanchkarata', 'vandemaataram', 'should', 'agsta', 'neu', 'dastakhatshuda', 'surinama', 'kingkhan', 'taras', 'rekannecting', 'phaibrik', 'clair', 'chhejat', 'mominatolaa', 'virodh', 'arena', 'unionwaadi', 'bamleshvaree', 'boca', 'immaturity', 'sansthaan', 'jin', 'dadrewa', 'kide', 'karawanou', 'putin', 'tankaar', 'khatakati', 'jadd', 'pushpa', 'gm', 'yusugi', 'korola', 'pyade', 'karavaayegee', 'balaatkaariyon', 'jatatee', 'gurdaspur', 'shreshthaswaroop', 'shubhprabhaa', 'europlast', 'rashtrabhakton', 'canterbari', 'communicators', 'mahamanch', 'conway', 'dwf', 'reeves', 'dabhoi', 'bhootal', 'insaano', 'emily', 'vigyaanam', 'sapatey', 'atwaal', 'hindan', 'aaufisaree', 'montclair', 'purnsveekrit', 'richardson', 'sandrbh', 'fargo', 'aleem', 'saptakranti', 'hr', 'imaandaari', 'kontrekting', 'gangajal', 'gaur', 'adhikariyon', 'bandhan', 'sendha', 'dablyoopidi', 'gravita', 'lincoln', 'pravrittivigyanon', 'cementation', 'bagad', 'gentry', 'datson', 'montebello', 'renewables', 'naksalwad', 'aitraz', 'itibhagwatee', 'niveshkarta', 'patkathayen', 'kaabiletaarif', 'bhaadas', 'bhogte', 'holding', 'dhatta', 'dhoron', 'upakhyanon', 'fayetteville', 'stockton', 'brijesh', 'pratigya', 'badhaati', 'then', 'thomas', 'dhyeywadi', 'tuesday', 'wgpl', 'ovum', 'gati', 'nucleus', 'kraar', 'purooshottamaachary', 'manchanda', 'lahsun', 'naqaab', 'suja', 'mckenzie', 'purzon', 'bahunt', 'bravo', 'amaanat', 'jhuthla', 'bush', 'chaukhate', 'apar', 'asthetic', 'kshetradwara', 'sajja', 'dhahaane', 'nariyalyukt', 'jealous', 'kursiyo', 'singhaadey', 'amy', 'antarmukh', 'thakkar', 'badbolepan', 'naxaliyon', 'manner', 'himali', 'naksalavaad', 'half', 'mahaadharmaadhyaksha', 'parmoujood', 'tippanikar', 'perels', 'bhapaee', 'beetaane', 'valve', 'dwirookta', 'eddie', 'jivaniyaan', 'panghal', 'khand', 'urvarkon', 'besin', 'giriwasiyon', 'paradhin', 'pukhtaa', 'achnera', 'carson', 'delta', 'tripura', 'alaymani', 'kheda', 'nahargarh', 'shalimar', 'immaichyority', 'elendt', 'properphasebuk', 'udanon', 'mhajan', 'moses', 'bataayaaki', 'alai', 'ilectromeetar', 'amod', 'rookenge', 'maariya', 'raagon', 'aaipief', 'phalitartha', 'supachy', 'awashist', 'ishat', 'rate', 'aata', 'mahalon', 'warishthon', 'jalapot', 'katrina', 'jismafaroshee', 'salary', 'raza', 'rectare', 'basil', 'badhaataa', 'charanbaddha', 'bhadkaya', 'sahityotsav', 'naraseepura', 'samjhunga', 'chandrapur', 'sarkaa', 'dhaah', 'praudh', 'ahitakaaree', 'darawani', 'belinda', 'alayamanee', 'datsun', 'ate', 'fasteners', 'bangravala', 'saira', 'atthas', 'nirdal', 'mahbubnagar', 'daratey', 'valkkan', 'khastaur', 'barsaai', 'ananddaayak', 'banbi', 'toh', 'asmanye', 'wipannta', 'spast', 'amzera', 'bechakar', 'ghatabadh', 'kaushalta', 'machine', 'wireless', 'fainn', 'gaano', 'khambhat', 'kyooaaeepeees', 'dublin', 'mta', 'samruddhiyon', 'radhakrushnadas', 'kaansting', 'idukki', 'industrial', 'bharatwanshee', 'maangi', 'robins', 'vladimir', 'pehowa', 'bagh', 'ummidwaaree', 'saafgoi', 'koottaa', 'adain', 'grenite', 'ustara', 'dhundhta', 'sangheeyataa', 'kumari', 'depthiriya', 'adchane', 'koriyaah', 'prapat', 'chhudawaaya', 'suiyan', 'veronika', 'schaeffler', 'jyaadaa', 'potter', 'media', 'yarushalem', 'pichde', 'letaseng', 'arthavyawasthataaon', 'kakoo', 'nirdeshika', 'seto', 'navate', 'samvedansheelata', 'roadrej', 'chhahon', 'laphj', 'vanshvad', 'rohtak', 'atakta', 'baawaraa', 'kribarabh', 'prakashakah', 'styrolution', 'apaardarshi', 'bengaluru', 'andhapan', 'mainan', 'kyaah', 'formulae', 'hamalavar', 'amavasya', 'purvapekshaaon', 'daishawaalaa', 'oklahoma', 'darmait', 'sahakalakar', 'fatma', 'khalati', 'dayitva', 'kyusaiks', 'tolakarmee', 'version', 'parisamvad', 'immune', 'bachti', 'utilities', 'malmaas', 'cadmium', 'antioch', 'khatedaar', 'mohini', 'vishwasniiyataa', 'rehman', 'kanblon', 'aaryika', 'marrej', 'chheenk', 'phalata', 'baagoraa', 'pakaude', 'lucie', 'fatehgadh', 'grahanakaal', 'jobs', 'sadanand', 'dutt', 'murgeepalan', 'lurhakate', 'palmonari', 'win', 'ubhaara', 'beck', 'padaraunaa', 'bandhate', 'kapoor', 'sevaala', 'vershan', 'chidhate', 'thane', 'nikalogi', 'mogali', 'dredger', 'nagarik', 'gajipur', 'vadak', 'mitchell', 'gyatvy', 'standhaari', 'dhoraaji', 'abdunnaasir', 'vishvwandhya', 'satyaarthprakash', 'shivamogga', 'ghumakkdi', 'gayen', 'chee', 'theateron', 'anadhikaarik', 'shasak', 'hines', 'bettiah', 'dhatna', 'rahamat', 'motihari', 'pearson', 'ujback', 'wheeling', 'pythogaras', 'wipro', 'kaabiletaareef', 'jamaate', 'pulam', 'daubaaraa', 'portillo', 'adhik', 'dhumrapaan', 'daampatya', 'jamudiya', 'pedoo', 'pipra', 'blair', 'communications', 'alayans', 'aradas', 'gudna', 'just', 'aawajon', 'benvar', 'amarillo', 'bramhaand', 'dikhai', 'tantro', 'aasan', 'tuglakabad', 'sutton', 'kandrour', 'khanna', 'nikalani', 'dibanoo', 'immachyoritee', 'tolaasan', 'jaanaakaree', 'liwaal', 'engeline', 'rushitaa', 'flint', 'vrishchikahsaptah', 'aasmatee', 'girkar', 'vaigyaanik', 'bishadhee', 'phoolatee', 'punahsthapana', 'badh', 'gandha', 'chakshu', 'miktaam', 'liidaron', 'zabardast', 'stuti', 'aurato', 'heblikar', 'rattebaajee', 'francisca', 'mumba', 'pratiyukti', 'darati', 'pranon', 'madison', 'nibhaaegee', 'myers', 'kashyap', 'brigadier', 'ambikaaon', 'hogaah', 'ishaak', 'sumi', 'deshbhaktimaya', 'dharmapatriyaan', 'krishnanagar', 'ea', 'aayudhnirmaanee', 'taaja', 'ajasra', 'fijulkharchi', 'mantrayon', 'tanta', 'dealership', 'chapel', 'dhwanimikon', 'education', 'jarakh', 'neet', 'jilawaasiyon', 'samprabhuta', 'kasegaa', 'pakanevala', 'deepn', 'kalasaanaa', 'mugaalataa', 'tokne', 'falibhoot', 'gyonth', 'dikhaai', 'shaamjeebhaaee', 'pahnaakar', 'kisaani', 'udhaaradaataaon', 'ramtek', 'wyaakhyaatmaktaa', 'somvaar', 'vista', 'polimer', 'raktaabh', 'bhugta', 'bhrashtachareeyon', 'khatakti', 'mukhyamantriyon', 'mad', 'kakshaang', 'jain', 'mahin', 'survana', 'baangarotiya', 'rasaayanik', 'aurora', 'chamchagiree', 'ncrwc', 'ainthey', 'essel', 'deshsewa', 'ilectrotecs', 'workcharge', 'kaarwaai', 'jhanktaa', 'urgent', 'sapraman', 'maurya', 'jini', 'jagadeeshachandra', 'asurakshit', 'kaavyoktiyaan', 'alnkaar', 'svadpremiyon', 'jaatak', 'rollers', 'ashaktataa', 'kaathiyavadee', 'aavriti', 'vidyaayein', 'vidhivat', 'raadhe', 'mukhyamantrisamvaaddaata', 'praansangalee', 'kanalog', 'pudukkottai', 'nicky', 'fuhadtaa', 'patachep', 'retreate', 'manovaigyaaniyon', 'stanadhari', 'mukhyopadhyaay', 'nirbaadh', 'todne', 'avuti', 'parikshankartaon', 'extactor', 'nashik', 'rasayanon', 'vaitravati', 'ogrenaaij', 'chausingha', 'phalodi', 'skhalan', 'panaamaa', 'nigar', 'monte', 'suneeti', 'shrabon', 'ambikaon', 'gujara', 'photofrem', 'prahlaad', 'beesavaan', 'dvivedi', 'denewaali', 'burleson', 'bharadwaj', 'jataati', 'malegaon', 'rahugaan', 'mahamantra', 'beast', 'divyata', 'sanjivan', 'kuposhit', 'eksclation', 'chadhega', 'texarkana', 'dungerpur', 'sarahi', 'laalaparee', 'aeronautics', 'worth', 'aasteen', 'svapnika', 'upakaaryaalayon', 'mimicry', 'ballabh', 'veriphication', 'hackensack', 'aakaash', 'bona', 'clock', 'daphan', 'pegav', 'kahlaya', 'madheekhedha', 'teleebag', 'dhaarak', 'nikaalani', 'kanghe', 'prashnottari', 'kakinada', 'bhaagyoday', 'singer', 'machaye', 'lekhakganon', 'uchhalne', 'dharmarajne', 'ainjal', 'bantata', 'pharm', 'ravel', 'hanumangarh', 'veeraangnaaon', 'gaytri', 'venkatraman', 'dibnoo', 'purooshottamachaary', 'sanyojakon', 'jhaauganj', 'sarkaren', 'newtest', 'shrikar', 'mitaao', 'diyotsiddh', 'bhoopsingh', 'charchaaen', 'prasarakon', 'nishpaap', 'flin', 'ranneetibanaa', 'eeaarjee', 'dhaagaa', 'gomukh', 'jud', 'khanikarmi', 'sharabh', 'arellano', 'ukhadhakar', 'lenevali', 'kumar', 'singhde', 'toncil', 'davindarpal', 'singaar', 'khisakate', 'enid', 'megapixal', 'digvijay', 'fazalon', 'doge', 'labelon', 'munafa', 'badhane', 'very', 'saatsera', 'ortega', 'rowe', 'bhatia', 'raajsthanwasiyon', 'baagbageechon', 'wilkinson', 'daraati', 'atke', 'pramukhataapoorwak', 'yatna', 'nitin', 'city', 'radico', 'dukanon', 'jalsansadhan', 'masyendranath', 'behar', 'barsheey', 'avru', 'daw', 'naqab', 'saregama', 'hugli', 'satti', 'neg', 'mahabaleshwar', 'placentia', 'varsaache', 'kumau', 'emi', 'theatrs', 'erika', 'khinchakar', 'raubadar', 'alto', 'kavyavidhaaon', 'coon', 'akron', 'tahkhaana', 'samajjanon', 'machaaye', 'suryadev', 'prathamik', 'pravritriyon', 'taajaa', 'jivanshaili', 'raktdaataaon', 'baleshwar', 'bhautikwaadiyon', 'lagaateen', 'jhaiyaan', 'vanshvaad', 'navanagar', 'najarandaaz', 'anshaankan', 'funkane', 'matpatron', 'module', 'farms', 'pahunchogee', 'maidi', 'thief', 'pfaudler', 'amit', 'poorvabhadrapad', 'landara', 'gonpo', 'vesle', 'shaao', 'hamidan', 'weeks', 'mineral', 'centennial', 'bastiyo', 'bagbagichon', 'pranit', 'bhedabhaavon', 'karaahanaa', 'greenville', 'sherawat', 'ralaud', 'jatakon', 'rishte', 'bentonville', 'najibabaad', 'financiers', 'phatahee', 'kerala', 'ladaaiyon', 'harijan', 'whitaker', 'vidisha', 'paul', 'aryon', 'khilana', 'chhutane', 'dukhiyon', 'poorneshwari', 'visabal', 'daramyanee', 'yari', 'vidhamaan', 'yatraein', 'sunteck', 'gujarata', 'niyanta', 'dum', 'awasthi', 'plus', 'bhen', 'stops', 'dhakelte', 'kpo', 'damghontu', 'insano', 'merraige', 'asfalt', 'laundry', 'ratnaabhooshan', 'ski', 'dushwaariyon', 'anupryogon', 'deshseva', 'lela', 'jagaragallu', 'mejbani', 'dehri', 'chkkar', 'crawford', 'guduri', 'dvc', 'prakriyagat', 'nag', 'matsyavatar', 'scootaron', 'ardhavishvas', 'daramyaanee', 'survanaa', 'sthitiyan', 'dharmaanand', 'sofner', 'squash', 'vidhansabhaen', 'randolph', 'mezbaan', 'sprit', 'sheelabhang', 'kalmein', 'swarnikaa', 'mandi', 'arthawyavasthataaon', 'kundaliya', 'nimkathana', 'farm', 'hatyaaropit', 'putta', 'faurensis', 'purooshottamachary', 'bonds', 'shikaayatkartaa', 'cronfrencing', 'ubal', 'baabaao', 'halmatpura', 'haqeeqat', 'barman', 'vartmaan', 'baantin', 'achanbha', 'jiwanparyant', 'wadara', 'sadaqen', 'aisa', 'jyotishyshastron', 'pythagoras', 'engelin', 'kapolkalpana', 'dropadiyon', 'rozlyn', 'sriram', 'gopaalak', 'bangbandhu', 'mistriyan', 'usay', 'mercado', 'maisore', 'offshor', 'kapurthala', 'sahovaliya', 'maladahiya', 'hymen', 'indaurnagar', 'visheshagyataon', 'usaki', 'caplin', 'mahapran', 'rupesh', 'reno', 'sudeepton', 'swamy', 'amara', 'vyavasthaamoolak', 'sensing', 'tyler', 'sangareddi', 'pehno', 'mardani', 'bamleshwaree', 'deewani', 'fincorp', 'navaa', 'shimat', 'sohawal', 'banaam', 'prawrutriyon', 'xiong', 'varsted', 'raajon', 'cheekhana', 'naimul', 'badhotri', 'antony', 'chhanyasa', 'nirdeshikaa', 'niptaaraa', 'judne', 'sujhati', 'pathalogist', 'tide', 'shrungar', 'braandes', 'parmeshvarpur', 'jedateeisaaft', 'uddhaatan', 'stan', 'aadivaadiyon', 'julaahon', 'maharaja', 'gatishilata', 'shipra', 'tribunal', 'khyber', 'wanvibhag', 'ghataatop', 'dharmagranthon', 'prawrittiyon', 'gaanth', 'mysore', 'sehgal', 'namkin', 'vaidh', 'vanaspatiyaan', 'jiangkou', 'pakhandi', 'protekts', 'elbaino', 'linden', 'sohaval', 'bhurbhura', 'gastrology', 'widroop', 'sajsajja', 'raatrichar', 'rojana', 'jalbhrav', 'montgomery', 'sev', 'bhrastrachariyon', 'dwn', 'streshan', 'ganthe', 'gehlot', 'chhateni', 'vishwasniiya', 'jivaniyan', 'dastaanen', 'dhona', 'kharuwaar', 'suriname', 'mangalsinh', 'anyonyashritta', 'ninties', 'paancholaas', 'reson', 'vaataanukooliton', 'chatushkoniya', 'aamdi', 'dushtaapurvak', 'bechate', 'chuka', 'mizoram', 'torrance', 'anuchit', 'avarodhak', 'ross', 'achyut', 'oodhami', 'haraval', 'gould', 'daaw', 'srijanvigyanee', 'nextgen', 'rukavaton', 'bhavanaon', 'dehdhaari', 'parikhaen', 'utaney', 'sanskrition', 'prospex', 'naeedilli', 'saayn', 'geneva', 'rasad', 'baramula', 'vandemataram', 'safari', 'chamba', 'sawal', 'bannari', 'fatehpur', 'pehnate', 'spray', 'normal', 'allout', 'manassas', 'mantriyonke', 'karyakaryakartaon', 'phaadanhe', 'greg', 'baitool', 'kushan', 'subbanee', 'infibeam', 'khatakhataakar', 'chachaji', 'chandrabali', 'baripada', 'roohelkhand', 'ghairakaanoonee', 'aarekhah', 'pramukhkarmiyon', 'lee', 'thiyeters', 'assam', 'faanon', 'jyada', 'theateron', 'safaikarmi', 'bieseses', 'cronfrensing', 'jeevraaj', 'thirake', 'shaksubahey', 'paratrooper', 'dabhara', 'usi', 'nihsandeh', 'mahanatam', 'banthia', 'anam', 'kashmir', 'baavadee', 'bhutto', 'maarake', 'kaaryayojna', 'francisko', 'adani', 'dhuriyaa', 'mhavidya', 'megapixel', 'cryptocurrencyon', 'muth', 'authoritee', 'aparivartanshil', 'pradhanmantriki', 'aagaashe', 'navjaagaran', 'attraa', 'ubala', 'amanati', 'khammam', 'haaranaa', 'bhavabhivyaktiyon', 'pushkar', 'shaantiyon', 'nerolac', 'chauka', 'siddhalingaa', 'bharatnatyam', 'javabah', 'dushwariyon', 'manchiy', 'jiyo', 'sapramaan', 'sim', 'peeechdablyooseees', 'bahav', 'tik', 'agavai', 'higman', 'qiwi', 'agyat', 'agende', 'lynchburg', 'maangeena', 'khatakhataaegee', 'jodhikaa', 'srujanvigyaani', 'detasan', 'badalwa', 'urf', 'chhaapemaar', 'lux', 'bhopal', 'phalibhoot', 'paithelaujist', 'paryaptta', 'furti', 'mrutyuniwaarak', 'maruti', 'uthaakar', 'alo', 'pegaw', 'pilibhit', 'somi', 'sanchalnon', 'kanchipuram', 'daraatee', 'yaddasht', 'sabudana', 'welding', 'veronica', 'stimberwala', 'mayajaal', 'pes', 'atul', 'outomaitically', 'jaanchana', 'mithilanchal', 'phiksingah', 'sajetee', 'plano', 'visangatiyaa', 'khhomche', 'lagna', 'mukeshwari', 'tasvaaron', 'dev', 'vesh', 'jyothy', 'landal', 'viraangnaaon', 'yanit', 'purnia', 'chauk', 'ladeen', 'chest', 'kasega', 'abut', 'renuka', 'loowe', 'gulaabaag', 'lajjalu', 'sabhaar', 'gairarashtravaadee', 'dhaavakon', 'lim', 'vidhaaein', 'svayamsveekrat', 'electrotex', 'ramraj', 'janbhavna', 'yauniktaa', 'haathe', 'dune', 'protects', 'vareinye', 'mazboori', 'praveenchand', 'vaigyanik', 'torrent', 'aatmayen', 'samprati', 'matthe', 'savita', 'atlantic', 'jalgti', 'chacchaa', 'afreen', 'bazaru', 'chauki', 'pharse', 'tuning', 'nodaraanee', 'maum', 'vidhaanee', 'gujar', 'pakadkar', 'aankhen', 'aapka', 'warner', 'anvaharya', 'likhehain', 'paathyakram', 'shavkon', 'yarooshaleim', 'mandalaayukto', 'naino', 'jhaaiyaan', 'simferopol', 'dhamm', 'nikharegaa', 'ghumaan', 'hounga', 'dhakal', 'sagarika', 'bernard', 'yogabal', 'breweries', 'dish', 'aithan', 'vartamaan', 'tremasik', 'tasvire', 'vyvhargat', 'maratha', 'banvaa', 'svarnika', 'prasath', 'overall', 'purwafalguni', 'properphasebook', 'suven', 'sapate', 'jh', 'karghe', 'deshkaal', 'nathdwara', 'thanvi', 'oran', 'serrano', 'ear', 'submission', 'dharak', 'tadad', 'greenwood', 'haute', 'behuria', 'niyantaaon', 'churu', 'missouri', 'anyaya', 'generat', 'nyutest', 'garud', 'swaastikakaar', 'shaie', 'siddhantavaadiyon', 'anawashyak', 'seaten', 'gulshan', 'riya', 'firmian', 'tyre', 'chi', 'badlapur', 'yuddhanaukaa', 'nbcc', 'shreeganganagarvaasiyon', 'udaa', 'cambridge', 'airway', 'idol', 'calumet', 'aajmaanaa', 'tantrikao', 'jonesboro', 'buddhijeewon', 'klozup', 'nmdc', 'raashtrapit', 'kamarhati', 'asphault', 'fransisko', 'bhalla', 'maulvidon', 'narwar', 'lahlahati', 'byaawa', 'ghataaegi', 'naamajadagee', 'takah', 'sevagram', 'laamiyan', 'gin', 'swechha', 'antadiyaan', 'arya', 'polishing', 'jokhimabharaa', 'umadatee', 'tantra', 'samoa', 'csi', 'mandya', 'lawrie', 'pahante', 'ankor', 'nimnavat', 'rapping', 'roopesh', 'pratibaddhata', 'nadep', 'sailaaneepan', 'menrajya', 'daav', 'aaboorod', 'pathyakramon', 'nividaataaon', 'gandhipura', 'phwcs', 'narayangadh', 'jaylalita', 'susanskrut', 'masood', 'woonsocket', 'sanskrutiyan', 'urff', 'theratipally', 'pratinirdesh', 'ulhaaspur', 'pahanate', 'dhawakon', 'bhagedh', 'pransangali', 'jalvayu', 'bhijvaai', 'sayra', 'krendron', 'maujud', 'siddhaant', 'kamyuniketars', 'chidchidapan', 'muthoot', 'wiwekadhikaron', 'maloun', 'cincinnati', 'registan', 'parosne', 'vichaarakon', 'janbhawna', 'andaman', 'badyal', 'morepen', 'beard', 'nodarani', 'dikhayee', 'chink', 'wyakhyatmakta', 'shaishav', 'aainthoo', 'ahemdabadah', 'jaayke', 'cigniti', 'kurukshetra', 'pravritteeyon', 'charcha', 'lakshmeechandra', 'hammond', 'nibhaegi', 'dhunai', 'janbhaavna', 'cant', 'melaghaat', 'mue', 'shwing', 'dampatya', 'kpada', 'santati', 'maanavtarhit', 'anuprayogi', 'harvey', 'kaju', 'cerritos', 'texmaco', 'sanchar', 'tuth', 'lozano', 'ropaney', 'hollywood', 'chhinke', 'dhamma', 'technoplast', 'pujapath', 'chanan', 'shrankhalaaon', 'chhednaa', 'carbogen', 'vyavadhan', 'schultz', 'istrailiyon', 'squash', 'danville', 'prana', 'thaakurvaadee', 'prayogshaastra', 'wyakteeyon', 'metrologist', 'madhura', 'paliyon', 'denver', 'apj', 'beni', 'pratisthaan', 'bhrashtaachaariyon', 'lakeer', 'sudhaargrah', 'sandali', 'jagriti', 'talwar', 'janashiksha', 'tapuwo', 'bulletin', 'ballebaajee', 'maadheekhedhaa', 'arthshaastriyon', 'scaining', 'mehman', 'rahegi', 'jamshedpur', 'tyauharon', 'darshaana', 'beechbachaav', 'sulati', 'varshan', 'sudhaaratee', 'wasaamukt', 'patil', 'jewaab', 'soto', 'dhtuen', 'dose', 'bhaagadaud', 'pagabadha', 'arushi', 'agusta', 'titan', 'equities', 'airtel', 'alaps', 'naksali', 'achivement', 'galaega', 'foodon', 'rajnichi', 'arya', 'streamwood', 'ghot', 'music', 'purkayastha', 'nora', 'tonic', 'gujaarne', 'water', 'sunsaan', 'sirsa', 'tangirala', 'hanson', 'varun', 'dreijer', 'doudaate', 'sharabon', 'vyakteeyon', 'saatseraa', 'darjano', 'agniyaan', 'dhonaa', 'karyakartriyon', 'awasthaen', 'himamaanav', 'two', 'ail', 'bandhuon', 'shraddhavanon', 'mariyo', 'vyaghat', 'michael', 'adrgh', 'vitthal', 'prarup', 'dhoondhti', 'dubana', 'ladne', 'form', 'zuari', 'pariwarwale', 'peranormal', 'bhuvneshvaprasad', 'iris', 'pratigyapoorn', 'lebalon', 'shreekrushnan', 'snikarse', 'kain', 'bhooswaami', 'karta', 'anyay', 'nokari', 'precision', 'chrom', 'jue', 'bowling', 'taapti', 'catholicon', 'nazarandaz', 'mulchand', 'sweccha', 'nanyaulaa', 'ghontana', 'advaitwad', 'underwood', 'lubhaavna', 'petroselee', 'amdi', 'ropane', 'daisy', 'jababee', 'cup', 'phaade', 'ulahna', 'unnatiyon', 'molina', 'gaandchubhonachoosnachhote', 'han', 'pradarshan', 'sarjuga', 'bidar', 'amargad', 'juye', 'tonk', 'devaapur', 'intelligence', 'neetiyon', 'khanaudaa', 'porbandar', 'sim', 'lagnesh', 'gaayikao', 'prishthabhoomiyon', 'moolchand', 'silchar', 'margarita', 'udakishuganj', 'sirtee', 'yamala', 'prashikshu', 'sierra', 'aalhaa', 'peck', 'taskhir', 'daka', 'nominate', 'sapat', 'coimbatore', 'koraput', 'svayattshasan', 'vidyutikrit', 'kikiyaanaa', 'arunaai', 'rigal', 'diamond', 'mridubhaashee', 'walters', 'sutrapada', 'keeti', 'zilaadhikaari', 'mangesh', 'pension', 'laganewale', 'jilavasiyon', 'ploskee', 'eksath', 'sekhar', 'dow', 'ardhawishwaas', 'rajasthan', 'vikaskhand', 'samshayon', 'badhanaa', 'classon', 'ikhrul', 'furaame', 'ebaz', 'baataaya', 'jushantan', 'strong', 'rajnitikaron', 'panipat', 'blankaard', 'jhabua', 'jacinto', 'barahsinga', 'suchnah', 'electromagic', 'lalmani', 'bhaaulaal', 'kathua', 'palaari', 'amitao', 'tadanrustee', 'vaabastaa', 'vlad', 'slash', 'waterbury', 'selfie', 'luis', 'panchasootra', 'aryama', 'vaadak', 'junction', 'generic', 'jiya', 'aden', 'sophner', 'qwvga', 'dhvanimikon', 'feeds', 'sukadiyaal', 'angadh', 'bavji', 'chhodiye', 'cipla', 'vigyanon', 'dhasmaanaa', 'longewal', 'badam', 'atraikt', 'saraaha', 'nrityashaili', 'sanvari', 'yakshadhipati', 'arabpatiyon', 'mukaddamon', 'phode', 'sihunta', 'jeewan', 'dhoondhata', 'cruces', 'kaaryakartriyon', 'pratibaddhtaaen', 'dlw', 'plan', 'pailot', 'adhiwas', 'phainn', 'favvara', 'halchal', 'wishvaweer', 'crest', 'brahmleen', 'virechan', 'yaaddaashta', 'ujjain', 'sahyaatre', 'jimme', 'sarawagi', 'dekhiyegaa', 'wajood', 'kundal', 'ria', 'slider', 'management', 'karyakaaryakartaon', 'cristis', 'ruktey', 'distrabyuters', 'spell', 'damapar', 'thikane', 'interactuality', 'cdyaan', 'south', 'aapkaa', 'nahinkarwaaee', 'jaagriti', 'choodhe', 'thirty', 'nadsa', 'gsm', 'gujarat', 'darshnarthee', 'nabha', 'dhokebaaj', 'lega', 'beeaaeeaaeeteeem', 'somwaar', 'wanon', 'audhyogikaran', 'agrahari', 'aafa', 'teemaardaron', 'misra', 'cerebra', 'naagrik', 'dastaar', 'makarh', 'gtor', 'ballistic', 'chadhegaa', 'maange', 'nilabh', 'mensharaab', 'singhade', 'hisaarah', 'ramkot', 'baithate', 'taskheer', 'ishaara', 'solapur', 'mejbaani', 'florence', 'paraamarshak', 'jhaadane', 'theaters', 'aadaan', 'ambience', 'surakshaah', 'hikaarico', 'baanyaa', 'rookvaai', 'mitravat', 'bokaro', 'maurya', 'pratinidhikaaree', 'chandramaon', 'parosane', 'dhaiya', 'spirits', 'leandro', 'sean', 'swarthparak', 'chuchiyaan', 'mahamidiya', 'poorwajanmon', 'shohadapantee', 'kushaan', 'jamloki', 'everest', 'taswaaron', 'advivas', 'premlata', 'murrieta', 'vaimanasya', 'kelly', 'tinplate', 'vaan', 'mahamantr', 'mezbaan', 'shumali', 'fulaaye', 'pruitt', 'rojvelt', 'prashasanik', 'ulazaanon', 'patrakaarvarta', 'duar', 'kathfodave', 'bipin', 'offerh', 'isayiyon', 'propellent', 'kansai', 'sikhin', 'cdcl', 'abbott', 'katlari', 'aadiwadiyon', 'chitrakathaa', 'avshisht', 'phyuchuristic', 'tereeh', 'nanhe', 'avery', 'suba', 'shakhayen', 'trikaldarshee', 'paalasaahee', 'failaiye', 'jhauvaa', 'jamati', 'janhani', 'ponding', 'taswaron', 'deshkal', 'hichkichaa', 'basteeh', 'garibi', 'phadphadaaen', 'sitaram', 'kmere', 'bairakpur', 'svayamswikrat', 'kaasovo', 'majboori', 'convocation', 'skipper', 'jaipur', 'prasthaanon', 'narm', 'radiopremi', 'titagarh', 'chico', 'rajyamantreesvatantra', 'mainval', 'gujarana', 'satluj', 'jatiyon', 'anhoni', 'telelinks', 'labs', 'harawal', 'marriage', 'maniya', 'tractors', 'kanchan', 'lairi', 'kaaryaawadhi', 'jalbharav', 'bhaadaa', 'baagora', 'piega', 'stephens', 'abhimat', 'accelya', 'wirerless', 'chauda', 'parbhani', 'khaatedar', 'badhna', 'burch', 'upadrawon', 'tikiyon', 'milk', 'dubona', 'baker', 'sutali', 'buddharam', 'wealth', 'patrani', 'ardhavishwaas', 'paim', 'castaneda', 'baljit', 'bhatnagar', 'jaatakon', 'ecuador', 'swargdoot', 'rushikul', 'tate', 'kanthhar', 'beaulad', 'europea', 'gaye', 'dadreva', 'bikherane', 'hamlo', 'pakdaae', 'uplabdhiyon', 'colorado', 'packard', 'jagruti', 'andhra', 'chounkate', 'bekhvaabiyaan', 'naphaa', 'sinha', 'utpidan', 'saga', 'aarya', 'dikhaaiye', 'anishtata', 'udan', 'sanyuktawastha', 'shamtal', 'evava', 'adhiwaktaagan', 'ralhan', 'jhuthalane', 'purvaabhaas', 'bijendrasinh', 'chinmani', 'loksbhavaali', 'ladki', 'narsinghgarh', 'sanghathanamantriyon', 'roopi', 'karykartaon', 'augusta', 'manmarji', 'antrmukh', 'anumar', 'rishtanaataa', 'dviarthi', 'chootile', 'hattiesburg', 'yoojee', 'raaj', 'raigulization', 'vindhya', 'fixingah', 'kamjor', 'bhara', 'chaudhavan', 'falne', 'shrunkhla', 'vipannta', 'proofread', 'zamana', 'ghataaegee', 'danga', 'erica', 'bhola', 'vyakhyatmakta', 'yojnah', 'webuniyadee', 'rumani', 'sahu', 'pradhaannagar', 'seattle', 'stain', 'insaani', 'hamla', 'nutrio', 'ladadd', 'antadiyan', 'kushwaha', 'kochchih', 'ganapathy', 'farmic', 'padengee', 'arth', 'mukarrar', 'niptakar', 'holidays', 'cooch', 'utthak', 'sulkhan', 'kamere', 'jagayega', 'santree', 'shakhsiyaten', 'panchang', 'zalana', 'nitya', 'ichchhee', 'habra', 'paaliyon', 'azoospermia', 'taavij', 'foodworks', 'graham', 'prabhaanmantree', 'seth', 'jagtap', 'harshit', 'dubna', 'raktadataon', 'mexico', 'tasir', 'eyarkandishnar', 'eddi', 'macon', 'israni', 'ainth', 'shreemahapurn', 'dork', 'jeenon', 'ghareebon', 'taraashakar', 'observer', 'aarya', 'ukhad', 'saathsaath', 'packagon', 'hindon', 'aprihary', 'manzil', 'lx', 'karvi', 'dubhashiyon', 'rikannekting', 'wind', 'client', 'shararat', 'ventura', 'rollins', 'haynes', 'niketan', 'krantiyon', 'twin', 'divya', 'gidgidate', 'nigmaayukt', 'gabalee', 'novato', 'rishwatkhor', 'faansi', 'garjana', 'nihlani', 'pintaa', 'greer', 'lewisville', 'prakaashit', 'clariant', 'khaayal', 'dwiarthi', 'raakhe', 'bulandshahr', 'stafford', 'najariyon', 'world', 'jeevniyaan', 'parmeshwarpur', 'uthanee', 'nyootriyo', 'jetha', 'baagbagichon', 'suchikitsaa', 'nagmandal', 'pradadhikariyon', 'urvarta', 'neal', 'kriptocurrencyon', 'svaayattshaasan', 'zarda', 'uplakshya', 'chhudwaya', 'vidagdhaa', 'mukhyamantripatana', 'gene', 'ratanpur', 'mhantam', 'guna', 'mtech', 'denevali', 'naabhikund', 'visheshanon', 'langaron', 'polythenon', 'sambit', 'dhundh', 'haan', 'takra', 'dhandhekhori', 'pulinde', 'epoklips', 'vigyaanapeediyaa', 'mahoney', 'poonjitantra', 'khare', 'ekstactor', 'zunheboto', 'priyamani', 'sawaal', 'sukhakar', 'aalochak', 'phaans', 'jayalalitaa', 'amjhera', 'bombay', 'vyagra', 'apnani', 'risane', 'chhure', 'charlotte', 'jose', 'raileeh', 'mejbaan', 'dhandhanya', 'dabur', 'bsss', 'najib', 'avsthayen', 'jabalpur', 'dharmanirpekshtaake', 'farmer', 'nacl', 'somerville', 'bull', 'jhulae', 'khagyaar', 'meharaam', 'dhasmana', 'ripan', 'bhagdaud', 'mithya', 'lalaaniyaa', 'bhulon', 'vadali', 'nagappattinam', 'vaimanya', 'biggest', 'jodta', 'mahattaron', 'orchha', 'dhairyavaan', 'sund', 'prince', 'zuban', 'camstudio', 'yooropiyaa', 'meza', 'bhusawal', 'gayatri', 'ware', 'buddhizeevon', 'enth', 'radar', 'ilayachi', 'ubaal', 'shreeganganagarwaasiyon', 'fitzgerald', 'jeevanshaili', 'chart', 'sargi', 'vedon', 'murmur', 'bhaate', 'wipkha', 'bayaane', 'madanaganja', 'packageon', 'varnankartaaon', 'jyada', 'lokotsaw', 'tewree', 'jhaugunj', 'malwan', 'tadanrusti', 'el', 'qabzaa', 'susanskrit', 'sandhol', 'aamariyapadha', 'haabban', 'bowling', 'rasme', 'emdablyupiel', 'vidhaaon', 'bowie', 'chennai', 'kamaljeet', 'singha', 'pensiondhariyon', 'barah', 'dubana', 'esetal', 'chouhan', 'phalne', 'siddhaarthah', 'apex', 'dalton', 'shrikhand', 'textiles', 'agarwal', 'mithyaachariyon', 'cement', 'kraanfrensing', 'sanskrutiyon', 'dhankkar', 'nizamabad', 'chhatenee', 'dushkriyaa', 'sasaram', 'chidhte', 'lamba', 'lautis', 'khaen', 'dhndhe', 'antayant', 'mass', 'baraamad', 'kremistry', 'uthanevale', 'lehna', 'siragitti', 'control', 'andaajee', 'cateline', 'pakshpat', 'fodo', 'franciska', 'prayogaprushth', 'krantidhara', 'daishwala', 'khurasan', 'quarantine', 'fond', 'pakshapat', 'putla', 'truth', 'sathsath', 'mohim', 'johnson', 'cpm', 'bataaya', 'palmer', 'tughlaqabad', 'apcotex', 'hairam', 'sarebajar', 'jerusalem', 'mold', 'warnankartaaon', 'hooper', 'waters', 'lubhaate', 'fefda', 'kamottejna', 'dampati', 'soya', 'barbartapurna', 'tadipar', 'lutf', 'tariko', 'chataa', 'pittman', 'pahla', 'multimedia', 'tyohaar', 'label', 'le', 'tirthatan', 'satuaa', 'vargas', 'dabolim', 'soochnaah', 'labelling', 'roseville', 'phaaeebrets', 'aalto', 'partanee', 'asmitaon', 'want', 'tadpatee', 'patel', 'kaaji', 'dostanapurna', 'kshetron', 'lynch', 'state', 'ragon', 'malhotra', 'pragyatantra', 'duneerti', 'crosse', 'avyavasayi', 'dhiryawaan', 'time', 'ashariri', 'chandradas', 'jyotishi', 'alexandria', 'bewakoofaanaa', 'sirpur', 'uzbek', 'kharuvaar', 'dharmaviheenataa', 'surendranagar', 'aalha', 'arunachal', 'krueger', 'bagli', 'davindarpaal', 'chaplusi', 'sanskariyon', 'engines', 'mendez', 'siyah', 'gujaarane', 'gajara', 'kavi', 'midnapore', 'vrutton', 'sirmon', 'vartman', 'ubhrega', 'pilot', 'narbhakshi', 'entreeks', 'dayton', 'badoo', 'allrounder', 'nishra', 'wadhawan', 'jensen', 'pali', 'trimasik', 'attraikt', 'vavidyaalaya', 'chandan', 'mega', 'mansfield', 'mattancheri', 'navketan', 'kamtar', 'pityooniya', 'purooshottampur', 'tapan', 'jubaan', 'enseeaardablyoosee', 'haimbalin', 'aaraam', 'yashpal', 'wokha', 'chhetri', 'omaxe', 'parween', 'kukreja', 'premphalah', 'narmi', 'shrinkhalaon', 'avrodhak', 'teeron', 'ganesha', 'jhada', 'anupama', 'talabgaar', 'bindal', 'zavala', 'aamriyapada', 'pratinidhikaree', 'halmatpuraa', 'boodam', 'macias', 'medical', 'respecting', 'ranjkta', 'da', 'suction', 'bhumi', 'tyoharon', 'rangate', 'uttra', 'penshandhaariyon', 'karykartaaon', 'vidyaaon', 'dafan', 'sarhind', 'khaaskar', 'raith', 'gudagaunw', 'sahab', 'udaanon', 'majesco', 'pushpa', 'keshari', 'bombi', 'kargil', 'takara', 'biji', 'chrome', 'stithi', 'gulf', 'kaaloniwaasiyon', 'aavritiyon', 'suhanubhooti', 'chamach', 'virodhabhasi', 'saabhaar', 'allcargo', 'harana', 'avyavasayee', 'pravinses', 'chhetthari', 'pratishthi', 'sapa', 'patki', 'parchiyon', 'prativad', 'liphaaphaa', 'darshana', 'garrison', 'aakandon', 'dhandon', 'distreebyootaree', 'khatri', 'panjika', 'baatah', 'ravaley', 'kangan', 'premaatmak', 'sipahi', 'notebandishuda', 'devlaha', 'halla', 'jhamaaten', 'massey', 'usak', 'pitamah', 'vivaranahbhaarateey', 'nagercoil', 'lakhkhi', 'achhi', 'satin', 'nagpur', 'basirhat', 'chhelai', 'ubarkar', 'sanyuktavastha', 'suri', 'bhapaai', 'vyaktigat', 'mhathir', 'flesh', 'rashtrak', 'raajbhaaee', 'hong', 'langron', 'subhaay', 'fund', 'flowers', 'uthane', 'badaa', 'mishra', 'shat', 'manhattan', 'sarguja', 'evening', 'grahankaal', 'bagali', 'neeoo', 'mundwa', 'tolasan', 'jedateeisaft', 'naagmandal', 'kanaal', 'machilipatnam', 'felix', 'hastaksharon', 'bhilai', 'contracting', 'qraar', 'jaun', 'bhaloo', 'fortis', 'girijagharon', 'jainon', 'arthavyavasthaah', 'oxys', 'khitabon', 'enka', 'chashma', 'batao', 'ghrutakumaaree', 'pratestent', 'bauer', 'lakshmidhar', 'siyasi', 'upyogitawadiyon', 'toogaa', 'samadjiyon', 'khurpaka', 'kumavat', 'mwp', 'suraj', 'neuroscientists', 'chandreshwaranagar', 'tanish', 'farukhnagar', 'awaazon', 'najraane', 'pashuchaaraa', 'webuniyadi', 'danbury', 'naidillee', 'chhalla', 'rowan', 'sheopur', 'rawlani', 'sanvaree', 'dhaak', 'masse', 'insurance', 'samprbhuta', 'raashtraa', 'jauhari', 'judai', 'badamaashon', 'khitaabon', 'swayam', 'aagah', 'dhindhwal', 'ghise', 'sappaanwaalee', 'nikharen', 'baadhit', 'brahmapur', 'baytown', 'koling', 'samadhiyon', 'yooniyanwaadi', 'chhapemar', 'gareebon', 'thousand', 'dasila', 'kanjentevaaitis', 'badhaon', 'lagega', 'durghatna', 'nitiyon', 'capdhari', 'atri', 'singam', 'vastvik', 'substensis', 'peelepan', 'uthnee', 'handiyaa', 'pratirakshi', 'lalkarta', 'dukaanon', 'nividaataon', 'sarvjaateeya', 'narsanharon', 'neemkathana', 'herrera', 'junun', 'ikai', 'jagdalpur', 'swiftlet', 'jalagati', 'pitts', 'ropi', 'adanga', 'alasi', 'chaukiyan', 'layton', 'vakrangee', 'development', 'shawnee', 'pardesiyon', 'bhoogol', 'bagichi', 'sivan', 'stray', 'aalsi', 'view', 'vishhin', 'kissagoi', 'mahanideshak', 'chikitsaevam', 'bhattacharya', 'trenton', 'kanyaein', 'priyadarshini', 'hukhookh', 'shulkah', 'rane', 'mahavidya', 'sanshyon', 'shreneeh', 'odom', 'benitez', 'tyohar', 'matiyaanaa', 'shavyaatra', 'rishtey', 'rehaana', 'qss', 'anaavashyak', 'rijhaayaa', 'brigade', 'sapane', 'chuneen', 'koshadhikaari', 'karab', 'paltane', 'karneshwardham', 'aaryanas', 'pratishthanon', 'karaikal', 'uski', 'vipreetataa', 'parikshao', 'fyuji', 'laaminaar', 'jutati', 'kaushik', 'bekaaboo', 'nano', 'orissa', 'tomar', 'ropee', 'thappadhabaajhee', 'baabat', 'attahasa', 'colan', 'pratileetar', 'bhusi', 'haliyaa', 'utpeedan', 'pope', 'karkhele', 'bahulya', 'aswbhavik', 'kathhnaeeyon', 'ramjeelaal', 'vasulta', 'mukhyamantri', 'maria', 'satmev', 'baksa', 'udar', 'man', 'buddhijivon', 'sudip', 'jodo', 'tasdeek', 'nikaalni', 'bhatkaanaa', 'harna', 'dillon', 'registerd', 'bhandara', 'lalaniya', 'lima', 'lalkrishna', 'ratnabhooshan', 'sparivar', 'sah', 'sametti', 'pyria', 'maamalonmen', 'braat', 'vaiisan', 'villa', 'linda', 'khariya', 'chellani', 'electronics', 'rishiraj', 'udaipur', 'pagla', 'ritukaant', 'kurmi', 'dastaanaa', 'emrald', 'ghumaaee', 'updraviyon', 'oklahoma', 'dharmaraajne', 'jhaohuee', 'uppidan', 'khemik', 'tugalki', 'duar', 'avi', 'jadet', 'taiboo', 'programmers', 'taylorsville', 'chadron', 'racer', 'bhakaa', 'andhabhakto', 'obrayan', 'krutya', 'peeran', 'najriyon', 'mayur', 'chaudhawan', 'swargadoot', 'feka', 'independet', 'tinkaa', 'gurdon', 'jaatiyon', 'samanwayaka', 'sube', 'chandana', 'praarambhikataa', 'trap', 'udant', 'dynamatic', 'vidyuteekrit', 'lalaniyaa', 'wadaraa', 'ved', 'fadfadaen', 'traded', 'akhuaapaadaa', 'saraha', 'vigyaanvaadiyon', 'kamzor', 'youngstown', 'aaryam', 'patkathaein', 'bajpai', 'jassee', 'nau', 'evav', 'lagataar', 'suman', 'padanee', 'mindtree', 'distributary', 'jankaron', 'shankacharaachaarya', 'offshore', 'dukhon', 'nesco', 'phooltee', 'tools', 'verger', 'dumghontu', 'videoh', 'rajman', 'aatmamugdh', 'rockford', 'mitra', 'devnaar', 'ocoee', 'uttariya', 'kainawij', 'galaxion', 'jivaniya', 'hakikat', 'eliminator', 'augustine', 'kar', 'bitane', 'daftarwale', 'bairaadh', 'kambalon', 'jwar', 'fatama', 'vives', 'dwt', 'dhaatuein', 'siyaha', 'pitne', 'bichchon', 'jita', 'sevala', 'bataur', 'aditya', 'kshetren', 'ankeny', 'chheter', 'mukarte', 'enthe', 'balty', 'khangaali', 'beeeseses', 'banjajasam', 'pushpit', 'linmarga', 'pereddy', 'bodara', 'focus', 'dootaay', 'sreeleathers', 'mahankmadaan', 'ghunsa', 'kaavyavidhaaon', 'kshatriyaadi', 'jaankaaron', 'rakhna', 'rishtedaaro', 'shravani', 'kubuddeen', 'jewellers', 'mehaman', 'rojlin', 'jaikara', 'chabutre', 'haradil', 'truck', 'neurovascular', 'khelaane', 'stration', 'lakshayatirth', 'nations', 'paathshaalaayein', 'nivedanah', 'lidaron', 'tanla', 'sevakon', 'poorvapekshaon', 'mani', 'somvar', 'pran', 'avegyanik', 'aviation', 'brown', 'naakaam', 'adhivas', 'janjagran', 'rajnandgaon', 'larry', 'bhalu', 'tareeko', 'chicopee', 'refractories', 'baramad', 'kav', 'emerald', 'vivekaadheen', 'fadfadaaen', 'kaylana', 'paints', 'pittsburgh', 'hokhat', 'chaurasiya', 'sacchidanand', 'seide', 'charon', 'rizvi', 'clay', 'danpati', 'doorgadh', 'devapur', 'manwaayaa', 'latakkar', 'mehmaan', 'niya', 'udhampur', 'ataher', 'chhupata', 'tungnath', 'raghupati', 'prakandh', 'changed', 'gangtok', 'cricket', 'francisco', 'bhagane', 'adangaa', 'fixingh', 'kamau', 'tikone', 'pam', 'tarikon', 'aagaman', 'kidwai', 'aashriton', 'raghavan', 'janeudhari', 'davenport', 'preranaapuree', 'mahasinh', 'lordaa', 'introduction', 'swabhavatah', 'grishmotsav', 'talaashegaa', 'saharsa', 'mckay', 'moshe', 'granthiyan', 'lohaati', 'aliance', 'tagde', 'chinke', 'siwan', 'thaki', 'letakar', 'hikariko', 'tyres', 'nrutyon', 'deerfield', 'charit', 'bethlehem', 'fineotex', 'asahi', 'pehan', 'dhanshyaam', 'faarensis', 'lahanaa', 'dwarikesh', 'dubaana', 'karaboriyon', 'golamuree', 'raakhnau', 'vipanak', 'punchang', 'breed', 'gullaravaalaa', 'awp', 'baji', 'harpaal', 'premeshwar', 'prapt', 'naksalwaad', 'kanthahar', 'oxide', 'khairati', 'deshke', 'sunaseer', 'aphasaron', 'seho', 'chabate', 'miti', 'belcha', 'saflata', 'shbana', 'khaatootolaa', 'shivastava', 'preranapuree']\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "51200\n",
            "4096\n",
            "4096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating tokenizied dataset here\n",
        "\n",
        "# Define a function to generate unique tokens for Hindi and English words from a dataset\n",
        "def uni_tok(dataset):\n",
        "  # Extract Hindi and English sentences from the dataset\n",
        "  hin = dataset['hin'].values\n",
        "  eng = dataset['eng'].values\n",
        "\n",
        "  # Initialize sets to store unique tokens for Hindi and English\n",
        "  hin_token = set()\n",
        "  eng_token = set()\n",
        "\n",
        "  # Iterate through pairs of Hindi and English sentences\n",
        "  for i, j in zip(eng, hin):\n",
        "    # Iterate through characters in Hindi sentence\n",
        "    for chare in j:\n",
        "      # Add each character to Hindi tokens set\n",
        "      hin_token.add(chare)\n",
        "    # Iterate through characters in English sentence\n",
        "    for chare in i:\n",
        "      # Add each character to English tokens set\n",
        "      eng_token.add(chare)\n",
        "\n",
        "  # Sort the sets and convert them to lists\n",
        "  hin_token = sorted(list(hin_token))\n",
        "  eng_token = sorted(list(eng_token))\n",
        "\n",
        "  # Return the lists of unique Hindi and English tokens\n",
        "  return hin_token, eng_token\n",
        "\n",
        "# Call the function to generate tokens for the training data\n",
        "hindi_token, english_token = uni_tok(train_data)\n",
        "\n",
        "# Print some statistics and samples of the generated tokens\n",
        "print(\"english_token : \", english_token)\n",
        "print(\"length of english words : \", len(english_token))\n",
        "print(\"hindi_token : \", hindi_token)\n",
        "print(\"length of hindi words : \", len(hindi_token))\n",
        "\n",
        "# \"Matraon\" ke liye alag se token hai mtlb ki"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOO8dojV7snM",
        "outputId": "37f9874b-3ff8-4520-a09a-bf42daf3a2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "english_token :  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "length of english words :  26\n",
            "hindi_token :  ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "length of hindi words :  64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating token map here\n",
        "\n",
        "def tokenize_map(language_tokens , english_tokens):\n",
        "\n",
        "    # Create English token map with characters as keys and their index + 1 as values\n",
        "    english_token_map = dict([(ch,i+1) for i,ch in enumerate(english_tokens)])\n",
        "\n",
        "    # Create language token map with characters as keys and their index + 1 as values\n",
        "    language_token_map = dict([(ch,i+1) for i,ch in enumerate(language_tokens)])\n",
        "\n",
        "    # Create a reverse language token map with index + 1 as keys and characters as values\n",
        "    reverse_language_token_map = dict([(i+1,ch) for i,ch in enumerate(language_tokens)])\n",
        "\n",
        "    # Adding blank space to both token maps with value 0\n",
        "    language_token_map[\" \"] = 0\n",
        "    english_token_map[\" \"] = 0\n",
        "\n",
        "    # Add special tokens for beginning and end of sentence in language token map\n",
        "    language_token_map[';']=65\n",
        "    language_token_map['.']=66\n",
        "\n",
        "    # Add special tokens for beginning and end of sentence in English token map\n",
        "    english_token_map[';']=27\n",
        "    english_token_map['.']=28\n",
        "\n",
        "    # Add <unk> token for unknown characters to language token maps\n",
        "    language_token_map['<unk>']=64\n",
        "\n",
        "    # Update the reverse language token map with special tokens\n",
        "    reverse_language_token_map[64]='<unk>'\n",
        "    reverse_language_token_map[65]=';'\n",
        "    reverse_language_token_map[66]='.'\n",
        "    reverse_language_token_map[0]=''\n",
        "\n",
        "    # Return the Marathi token map, English token map, and reverse Marathi token map\n",
        "    return language_token_map, reverse_language_token_map, english_token_map\n",
        "\n",
        "# Call the function to generate token maps for hindi and english\n",
        "hin_token_map, reverse_hin_token_map, eng_token_map = tokenize_map(hindi_token, english_token)\n",
        "\n",
        "# Print the generated token maps\n",
        "print(hin_token_map)\n",
        "print(reverse_hin_token_map)\n",
        "print(eng_token_map)\n",
        "\n",
        "# Print the lengths of Marathi and English token maps\n",
        "print('hindi token map length:', len(hin_token_map))\n",
        "print('english token map length:', len(eng_token_map))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP1SUzLZZx1X",
        "outputId": "d740ce72-fdf8-4725-925e-e0c31b18557c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'': 1, '': 2, '': 3, '': 4, '': 5, '': 6, '': 7, '': 8, '': 9, '': 10, '': 11, '': 12, '': 13, '': 14, '': 15, '': 16, '': 17, '': 18, '': 19, '': 20, '': 21, '': 22, '': 23, '': 24, '': 25, '': 26, '': 27, '': 28, '': 29, '': 30, '': 31, '': 32, '': 33, '': 34, '': 35, '': 36, '': 37, '': 38, '': 39, '': 40, '': 41, '': 42, '': 43, '': 44, '': 45, '': 46, '': 47, '': 48, '': 49, '': 50, '': 51, '': 52, '': 53, '': 54, '': 55, '': 56, '': 57, '': 58, '': 59, '': 60, '': 61, '': 62, '': 63, '': 64, ' ': 0, ';': 65, '.': 66, '<unk>': 64}\n",
            "{1: '', 2: '', 3: '', 4: '', 5: '', 6: '', 7: '', 8: '', 9: '', 10: '', 11: '', 12: '', 13: '', 14: '', 15: '', 16: '', 17: '', 18: '', 19: '', 20: '', 21: '', 22: '', 23: '', 24: '', 25: '', 26: '', 27: '', 28: '', 29: '', 30: '', 31: '', 32: '', 33: '', 34: '', 35: '', 36: '', 37: '', 38: '', 39: '', 40: '', 41: '', 42: '', 43: '', 44: '', 45: '', 46: '', 47: '', 48: '', 49: '', 50: '', 51: '', 52: '', 53: '', 54: '', 55: '', 56: '', 57: '', 58: '', 59: '', 60: '', 61: '', 62: '', 63: '', 64: '<unk>', 65: ';', 66: '.', 0: ''}\n",
            "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, ' ': 0, ';': 27, '.': 28}\n",
            "hindi token map length: 68\n",
            "english token map length: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets see what is the maximun word length avaiolable in the dataset\n",
        "\n",
        "# Assigning values from the 'hin' and 'eng' columns of the test_data DataFrame to variables 'hiii' and 'ennn' respectively.\n",
        "hiii = test_data['hin'].values\n",
        "ennn = test_data['eng'].values\n",
        "\n",
        "# Adding ';' at the beginning and '.' at the end of each element in the 'hiii' and 'ennn' arrays.\n",
        "# Adding special characters like ';' and '.' the start and end of a sentence or phrase, which is necessary for subsequent processing or analysis being performed on the strings.\n",
        "hiii = ';' + hiii + '.'\n",
        "ennn = ';' + ennn + '.'\n",
        "\n",
        "#print(hiii)\n",
        "\n",
        "# Finding the maximum length of a string in the 'hiii' array.\n",
        "maximum_hin = max([len(i) for i in hiii])\n",
        "# Finding the maximum length of a string in the 'ennn' array.\n",
        "maximum_eng = max([len(i) for i in ennn])\n",
        "\n",
        "# Printing the maximum word length in Hindi.\n",
        "print(\"maximum word length in hindi is : \", maximum_hin)\n",
        "# Printing the maximum word length in English.\n",
        "print(\"maximum word length in english is : \", maximum_eng)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t1NmzzG-FEi",
        "outputId": "bcc44ca2-ee88-467f-8814-1b989cd3dddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maximum word length in hindi is :  22\n",
            "maximum word length in english is :  28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOw we do one hot encoding\n",
        "\n",
        "import torch\n",
        "#unknown token present in validation set as 'r.(in hindi)'\n",
        "unknown_token=64\n",
        "def process(data):\n",
        "    x,y = data['eng'].values, data['hin'].values\n",
        "    x = \";\" + x + \".\"\n",
        "    y = \";\" + y + \".\"\n",
        "    print(x[0:3])\n",
        "    print(y[0:3])\n",
        "\n",
        "    a = torch.zeros((len(x),maximum_eng),dtype=torch.int64)\n",
        "    print(a.shape)\n",
        "\n",
        "    b = torch.zeros((len(y),maximum_eng),dtype=torch.int64)\n",
        "\n",
        "    data=[]\n",
        "    for i,(xx,yy) in enumerate(zip(x,y)):\n",
        "        for j,ch in enumerate(xx):\n",
        "            a[i,j] = eng_token_map[ch]\n",
        "\n",
        "        #a[i,j+1:] = eng_token_map[\" \"]\n",
        "        for j,ch in enumerate(yy):\n",
        "            if ch in hin_token_map:\n",
        "             b[i,j] = hin_token_map[ch]\n",
        "            else:\n",
        "              b[i,j]= unknown_token\n",
        "\n",
        "\n",
        "    '''\n",
        "    data = []\n",
        "    for xx, yy in zip(x, y):\n",
        "        a_seq = [eng_token_map[ch] for ch in xx]\n",
        "        b_seq = [hin_token_map.get(ch, unknown_token) for ch in yy]\n",
        "        data.append((a_seq, b_seq))\n",
        "    '''\n",
        "\n",
        "    data = [(a[i], b[i]) for i in range(len(x))]\n",
        "    print(a.shape)\n",
        "    print(b.shape)\n",
        "    return data\n",
        "\n",
        "print(train_data)\n",
        "train_procs = process(train_data)\n",
        "valid_procs = process(valid_data)\n",
        "test_procs = process(test_data)\n",
        "# print(train_process.shape)\n",
        "print('\\n')\n",
        "print('num of rows:',len(train_procs))\n",
        "print('num of columns:',len(train_procs[0]))\n",
        "print(train_procs[0][1])\n",
        "#print(valid_procs[0][1])\n",
        "#print(test_procs[0][1])\n",
        "\n"
      ],
      "metadata": {
        "id": "Y50ML0xPARbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdb7a3fd-6fc8-4047-bb55-54356f92937a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               eng         hin\n",
            "0      shastragaar  \n",
            "1          bindhya    \n",
            "2        kirankant    \n",
            "3      yagyopaveet   \n",
            "4          ratania     \n",
            "...            ...         ...\n",
            "51195        toned        \n",
            "51196   mutanaazaa    \n",
            "51197    asahmaton     \n",
            "51198    sulgaayin    \n",
            "51199  anchuthengu   \n",
            "\n",
            "[51200 rows x 2 columns]\n",
            "[';shastragaar.' ';bindhya.' ';kirankant.']\n",
            "[';.' ';.' ';.']\n",
            "torch.Size([51200, 28])\n",
            "torch.Size([51200, 28])\n",
            "torch.Size([51200, 28])\n",
            "[';jaisawal.' ';bajai.' ';sanghthan.']\n",
            "[';.' ';.' ';.']\n",
            "torch.Size([4096, 28])\n",
            "torch.Size([4096, 28])\n",
            "torch.Size([4096, 28])\n",
            "[';thermax.' ';sikhaaega.' ';learn.']\n",
            "[';.' ';.' ';.']\n",
            "torch.Size([4096, 28])\n",
            "torch.Size([4096, 28])\n",
            "torch.Size([4096, 28])\n",
            "\n",
            "\n",
            "num of rows: 51200\n",
            "num of columns: 2\n",
            "tensor([65, 46, 48, 64, 31, 64, 42, 52, 18, 52, 42, 66,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For reading the words, bro...\n",
        "\n",
        "def reverse_tokenize(data):\n",
        "    # Convert each element in data to an integer\n",
        "    data = map(int, data)\n",
        "    # Map each integer to its corresponding character using reverse_marathi_token_map\n",
        "    characters = map(reverse_hin_token_map.get, data)\n",
        "    # Join the characters into a single string\n",
        "    predicted_seq = ''.join(characters)\n",
        "    return predicted_seq"
      ],
      "metadata": {
        "id": "VvnAb8a5BQzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_bitch_SIZE = 16\n",
        "\n",
        "PAD_IDX = 0\n",
        "BOS_IDX = ';'\n",
        "EOS_IDX = '.'\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#WE ARE NOT USING GENERATE BATCH FUNCTION\n",
        "'''\n",
        "def generate_batch(data_batch):\n",
        "  ma_batch, en_batch = [], []\n",
        "  for (en_item,ma_item) in data_batch:\n",
        "    #ma_batch.append(torch.cat([torch.tensor(mar_token_map[BOS_IDX]), ma_item, torch.tensor(mar_token_map[EOS_IDX])], dim=0))\n",
        "    #en_batch.append(torch.cat([torch.tensor(eng_token_map[BOS_IDX]), en_item, torch.tensor(eng_token_map[EOS_IDX])], dim=0))\n",
        "    #print(ma_item[0])\n",
        "    ma_batch=torch.tensor(ma_item,dtype=torch.int64)\n",
        "    en_batch=torch.tensor(en_item,dtype=torch.int64)\n",
        "\n",
        "  return  en_batch.to(device),ma_batch.to(device)\n",
        "\n",
        "'''\n",
        "\n",
        "train_itersn = DataLoader(train_procs, batch_size=BATCH_bitch_SIZE,\n",
        "                        shuffle=False)\n",
        "valid_itersn = DataLoader(valid_procs, batch_size=BATCH_bitch_SIZE,\n",
        "                        shuffle=False)\n",
        "test_itersn = DataLoader(test_procs, batch_size=BATCH_bitch_SIZE,\n",
        "                       shuffle=False)\n",
        "\n",
        "# Let's see what we got\n",
        "print(len(train_itersn))\n",
        "print(train_itersn)\n",
        "print(len(test_itersn))\n",
        "print(len(valid_itersn))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUiH0-giEBGS",
        "outputId": "af117c8d-55a1-4f6a-e390-8e1c51c342da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3200\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x000001976520C340>\n",
            "256\n",
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import string\n",
        "import random\n",
        "from collections import Counter\n",
        "# Set random seed for reproducibility\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "import os\n"
      ],
      "metadata": {
        "id": "aFI61OHpGqhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dimension: int, emb_dimension: int, enc_hid_dimension: int, dec_hid_dimension: int, dropout: float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dimension = input_dimension\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.enc_hid_dimension = enc_hid_dimension\n",
        "        self.dec_hid_dimension = dec_hid_dimension\n",
        "        self.dropout = dropout\n",
        "\n",
        "        #print('inp. dim')\n",
        "        #print(self.input_dim)\n",
        "        #print('emb. dim')\n",
        "        #print(self.emb_dim)\n",
        "        self.embedding = nn.Embedding(self.input_dimension, self.emb_dimension)\n",
        "\n",
        "\n",
        "        self.rnn = nn.GRU(emb_dimension, enc_hid_dimension, bidirectional = True)\n",
        "\n",
        "        self.fc = nn.Linear(enc_hid_dimension * 2, dec_hid_dimension)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src: Tensor) -> Tuple[Tensor]:\n",
        "        src = src.permute(1,0)\n",
        "        #print(src)\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "\n",
        "        return outputs, hidden\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dimension: int, dec_hid_dimension: int, attn_dimension: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc_hid_dimension = enc_hid_dimension\n",
        "        self.dec_hid_dimension = dec_hid_dimension\n",
        "\n",
        "        self.attn_in = (enc_hid_dimension * 2) + dec_hid_dimension\n",
        "\n",
        "        self.attn = nn.Linear(self.attn_in, attn_dimension)\n",
        "\n",
        "    def forward(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor:\n",
        "\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        energy = torch.tanh(self.attn(torch.cat((\n",
        "            repeated_decoder_hidden,\n",
        "            encoder_outputs),\n",
        "            dim = 2)))\n",
        "\n",
        "        attention = torch.sum(energy, dim=2)\n",
        "\n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dimension: int, emb_dimension: int, enc_hid_dimension: int, dec_hid_dimension: int, dropout: int, attention: nn.Module):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.enc_hid_dimension = enc_hid_dimension\n",
        "        self.dec_hid_dimension = dec_hid_dimension\n",
        "        self.output_dimension = output_dimension\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dimension, emb_dimension)\n",
        "\n",
        "        self.rnn = nn.GRU((enc_hid_dimension * 2) + emb_dimension, dec_hid_dimension)\n",
        "\n",
        "        self.out = nn.Linear(self.attention.attn_in + emb_dimension, output_dimension)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def _weighted_encoder_rep(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor:\n",
        "\n",
        "        a = self.attention(decoder_hidden, encoder_outputs)\n",
        "\n",
        "        a = a.unsqueeze(1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
        "\n",
        "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
        "\n",
        "        return weighted_encoder_rep\n",
        "\n",
        "\n",
        "    def forward(self, input_: Tensor, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
        "\n",
        "        input_ = input_.unsqueeze(0)\n",
        "        input_ = input_.permute(1,0)\n",
        "        embedded = self.dropout(self.embedding(input_))\n",
        "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n",
        "                                                          encoder_outputs)\n",
        "\n",
        "        #print(weighted_encoder_rep.shape)\n",
        "        embedded = embedded.permute(1,0,2)\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
        "\n",
        "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
        "\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
        "\n",
        "        output = self.out(torch.cat((output, weighted_encoder_rep, embedded), dim = 1))\n",
        "\n",
        "        return output, decoder_hidden.squeeze(0)\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder: nn.Module, decoder: nn.Module, device: torch.device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src: Tensor, trg: Tensor, teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        max_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dimension\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        #ATTENTION HEAT MAP METHOD\n",
        "        #attentions = torch.zeros(max_len, batch_size, src.shape[1]).to(self.device)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        # first input to the decoder is the <sos> token\n",
        "        trg = trg.permute(1,0)\n",
        "        output = trg[0,:]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (trg[t] if teacher_force else top1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "INPUT_DIMENSION = len(eng_token_map)\n",
        "OUTPUT_DIMENSION = len(hin_token_map)\n",
        "ENC_EMB_DIMENSION = 512\n",
        "DEC_EMB_DIMENSION = 512\n",
        "ENC_HID_DIMENSION  = 256\n",
        "DEC_HID_DIMENSION  = 256\n",
        "ATTN_DIMENSION  = 256\n",
        "ENC_DROPOUT = 0.3\n",
        "DEC_DROPOUT = 0.3\n",
        "\n",
        "encoder_bkl = Encoder(INPUT_DIMENSION , ENC_EMB_DIMENSION, ENC_HID_DIMENSION, DEC_HID_DIMENSION, ENC_DROPOUT)\n",
        "attension_bkl = Attention(ENC_HID_DIMENSION, DEC_HID_DIMENSION, ATTN_DIMENSION)\n",
        "decoder_bkl = Decoder(OUTPUT_DIMENSION , DEC_EMB_DIMENSION,  ENC_HID_DIMENSION, DEC_HID_DIMENSION, DEC_DROPOUT, attension_bkl)\n",
        "model_bkl = Seq2Seq(encoder_bkl, decoder_bkl, device).to(device)\n",
        "\n",
        "\n",
        "def init_weights(m: nn.Module):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "#added custom weights\n",
        "model_bkl.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(model_bkl.parameters())\n",
        "\n",
        "\n",
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f'The model has {count_parameters(model_bkl):,} trainable parameters')\n",
        "print(\"That's alot of parameters!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbIItUKpwflu",
        "outputId": "5fb5f8a0-69f0-4c1c-a209-472e9d7b22f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 2,632,260 trainable parameters\n",
            "That's alot of parameters!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wandb implimenststion of Attantion"
      ],
      "metadata": {
        "id": "b9Rd9asq8Wk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login 3c21150eb43b007ee446a1ff6e87f640ec7528c4 #my API key for wandb login\n",
        "import wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qNbTSZcFxOk",
        "outputId": "06c899de-fb1f-4703-d69d-9614b9ca8fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.0-py3-none-win_amd64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from wandb) (3.10.0)\n",
            "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb)\n",
            "  Downloading protobuf-4.25.3-cp38-cp38-win_amd64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from wandb) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.1.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp38-cp38-win_amd64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: setuptools in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from wandb) (68.2.2)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from wandb) (4.9.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
            "Requirement already satisfied: six>=1.4.0 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.0-py3-none-win_amd64.whl (6.7 MB)\n",
            "   ---------------------------------------- 0.0/6.7 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/6.7 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/6.7 MB 991.0 kB/s eta 0:00:07\n",
            "   ---------------------------------------- 0.1/6.7 MB 491.5 kB/s eta 0:00:14\n",
            "    --------------------------------------- 0.1/6.7 MB 877.7 kB/s eta 0:00:08\n",
            "   - -------------------------------------- 0.2/6.7 MB 935.2 kB/s eta 0:00:07\n",
            "   - -------------------------------------- 0.3/6.7 MB 1.2 MB/s eta 0:00:06\n",
            "   -- ------------------------------------- 0.5/6.7 MB 1.4 MB/s eta 0:00:05\n",
            "   --- ------------------------------------ 0.5/6.7 MB 1.4 MB/s eta 0:00:05\n",
            "   --- ------------------------------------ 0.6/6.7 MB 1.4 MB/s eta 0:00:05\n",
            "   ---- ----------------------------------- 0.7/6.7 MB 1.4 MB/s eta 0:00:05\n",
            "   ---- ----------------------------------- 0.8/6.7 MB 1.5 MB/s eta 0:00:05\n",
            "   ----- ---------------------------------- 1.0/6.7 MB 1.7 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 1.1/6.7 MB 1.7 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 1.2/6.7 MB 1.8 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 1.3/6.7 MB 1.8 MB/s eta 0:00:04\n",
            "   -------- ------------------------------- 1.4/6.7 MB 1.8 MB/s eta 0:00:03\n",
            "   -------- ------------------------------- 1.5/6.7 MB 1.8 MB/s eta 0:00:03\n",
            "   --------- ------------------------------ 1.5/6.7 MB 1.8 MB/s eta 0:00:03\n",
            "   --------- ------------------------------ 1.5/6.7 MB 1.8 MB/s eta 0:00:03\n",
            "   --------- ------------------------------ 1.6/6.7 MB 1.7 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 1.6/6.7 MB 1.7 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 1.7/6.7 MB 1.6 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 1.8/6.7 MB 1.6 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 1.9/6.7 MB 1.6 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 1.9/6.7 MB 1.6 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 2.0/6.7 MB 1.5 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 2.0/6.7 MB 1.5 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 2.0/6.7 MB 1.5 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 2.1/6.7 MB 1.5 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 2.2/6.7 MB 1.5 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 2.2/6.7 MB 1.5 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 2.3/6.7 MB 1.5 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 2.4/6.7 MB 1.5 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 2.4/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 2.5/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 2.5/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 2.6/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 2.7/6.7 MB 1.5 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 2.7/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 2.8/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 2.9/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 2.9/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 3.0/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 3.1/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 3.1/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 3.2/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 3.2/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 3.3/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 3.4/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 3.4/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 3.4/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 3.5/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 3.6/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 3.6/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 3.7/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 3.7/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 3.7/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 3.9/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 3.9/6.7 MB 1.4 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 4.1/6.7 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 4.1/6.7 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 4.2/6.7 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 4.4/6.7 MB 1.4 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 4.5/6.7 MB 1.4 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 4.6/6.7 MB 1.4 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 4.7/6.7 MB 1.5 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 4.8/6.7 MB 1.5 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 4.9/6.7 MB 1.5 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 5.1/6.7 MB 1.5 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 5.1/6.7 MB 1.5 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 5.3/6.7 MB 1.5 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 5.4/6.7 MB 1.5 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 5.6/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 5.7/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 5.8/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 5.9/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 6.0/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 6.1/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 6.3/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 6.3/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 6.3/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 6.5/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  6.7/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  6.7/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  6.7/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  6.7/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  6.7/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  6.7/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  6.7/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  6.7/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  6.7/6.7 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 6.7/6.7 MB 1.5 MB/s eta 0:00:00\n",
            "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "   ---------------------------------------- 0.0/207.3 kB ? eta -:--:--\n",
            "   ----- --------------------------------- 30.7/207.3 kB 660.6 kB/s eta 0:00:01\n",
            "   --------------------- ------------------ 112.6/207.3 kB 1.3 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 153.6/207.3 kB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 207.3/207.3 kB 1.2 MB/s eta 0:00:00\n",
            "Downloading protobuf-4.25.3-cp38-cp38-win_amd64.whl (413 kB)\n",
            "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
            "   --- ------------------------------------ 41.0/413.4 kB 1.9 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 143.4/413.4 kB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 317.4/413.4 kB 2.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  409.6/413.4 kB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 413.4/413.4 kB 1.8 MB/s eta 0:00:00\n",
            "Downloading sentry_sdk-2.1.1-py2.py3-none-any.whl (277 kB)\n",
            "   ---------------------------------------- 0.0/277.3 kB ? eta -:--:--\n",
            "   ---------------- ----------------------- 112.6/277.3 kB 6.4 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 204.8/277.3 kB 2.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 277.3/277.3 kB 1.9 MB/s eta 0:00:00\n",
            "Downloading setproctitle-1.3.3-cp38-cp38-win_amd64.whl (11 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "   ---------------------------------------- 0.0/62.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 62.7/62.7 kB 1.1 MB/s eta 0:00:00\n",
            "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, protobuf, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 protobuf-4.25.3 sentry-sdk-2.1.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\ASL 5\\.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "sweep_config = {\n",
        "    'method': 'bayes', #grid, random,bayes\n",
        "    'metric': {\n",
        "      'name': 'valid_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'embed_dimension': {\n",
        "            'values': [128,256,512]\n",
        "        },\n",
        "        'hidden_layer_dimension': {\n",
        "            'values': [128,256,512]\n",
        "        },\n",
        "        'attention_dimension':{\n",
        "            'values':[64,128,256]\n",
        "        },\n",
        "        'dropout':{\n",
        "            'values':[0.3,0.5,0.6]\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity='ed23d015', project=\"DL_Assignment_3\")\n",
        "\n",
        "def sweep_train():\n",
        "  # Default values for hyper-parameters we're going to sweep over\n",
        "  config_defaults = {\n",
        "      'embed_dimension':256,\n",
        "      'hidden_layer_dimension':256,\n",
        "      'attention_dimension':128,\n",
        "      'dropout':0.6,\n",
        "  }\n",
        "\n",
        "  # Initialize a new wandb run\n",
        "  wandb.init(project='DL_Assignment_3', entity='ed23d015',config=config_defaults)\n",
        "  wandb.run.name = 'embed_dimension:'+ str(wandb.config.embed_dimension)+' ;hl:'+str(wandb.config.hidden_layer_dimension)+ ' ;attention_dimension:'+str(wandb.config.attention_dimension)+ ' ;dropout:'+str(wandb.config.dropout)\n",
        "  config = wandb.config\n",
        "  embed_dimension = config.embed_dimension\n",
        "  hidden_layer_dimension = config.hidden_layer_dimension\n",
        "  attention_dimension = config.attention_dimension\n",
        "  dropout = config.dropout\n",
        "\n",
        "  # Doing Model training here\n",
        "  INPUT_DIM = len(eng_token_map)\n",
        "  OUTPUT_DIM = len(hin_token_map)\n",
        "\n",
        "\n",
        "  encoder_bkl = Encoder(INPUT_DIM, embed_dimension, hidden_layer_dimension, hidden_layer_dimension, dropout)\n",
        "\n",
        "  attension_bkl = Attention(hidden_layer_dimension, hidden_layer_dimension, attention_dimension)\n",
        "\n",
        "  decoder_bkl = Decoder(OUTPUT_DIM, embed_dimension, hidden_layer_dimension, hidden_layer_dimension, dropout, attension_bkl)\n",
        "\n",
        "  model_bkl = Seq2Seq(encoder_bkl, decoder_bkl, device).to(device)\n",
        "  model_bkl.apply(init_weights)\n",
        "  optimizer = optim.Adam(model_bkl.parameters())\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  model_bkl.train()\n",
        "\n",
        "  # Initializing losses and accuracies\n",
        "  training_epoch_loss = 0\n",
        "  training_epoch_accuracy = 0\n",
        "  validation_epoch_loss = 0\n",
        "  validation_epoch_accuracy = 0\n",
        "\n",
        "  N_EPOCHS = 10\n",
        "  CLIP = 1\n",
        "\n",
        "  for epoch in range(N_EPOCHS):\n",
        "\n",
        "  #TRAINING BLOCK\n",
        "    model_bkl.train()\n",
        "    for _, (source, target) in enumerate(train_itersn):\n",
        "        source, target = source.to(device), target.to(device)\n",
        "        #print(\"target1: \", target)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model_bkl(source, target)\n",
        "\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        target = target.permute(1,0)\n",
        "        target = torch.reshape(target[1:], (-1,))\n",
        "        #print(\"target2: \", target.shape)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # calculating accuracy\n",
        "        preds = torch.argmax(output, dim=1)\n",
        "        #print(\"pred: \", preds)\n",
        "        #print(\"target3: \", target)\n",
        "        non_pad_elements = (target != 0).nonzero(as_tuple=True)[0]\n",
        "        #print(\"non_pad_elements: \", non_pad_elements)\n",
        "        train_correct = preds[non_pad_elements] == target[non_pad_elements]\n",
        "\n",
        "        #print(\"len(non_pad_elements): \", len(non_pad_elements))\n",
        "        training_epoch_accuracy += train_correct.sum().item() / len(non_pad_elements)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model_bkl.parameters(), CLIP)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        training_epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "    training_epoch_loss = training_epoch_loss / len(train_itersn)\n",
        "    training_epoch_accuracy = training_epoch_accuracy / len(train_itersn)\n",
        "\n",
        "\n",
        "    #EVALUATION MODE ON\n",
        "    model_bkl.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, (source, target) in enumerate(valid_itersn):\n",
        "            source, target = source.to(device), target.to(device)\n",
        "\n",
        "            output = model_bkl(source, target, 0) # Turn off teacher forcing\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            target = target.permute(1,0)\n",
        "            target = torch.reshape(target[1:], (-1,))\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            validation_epoch_loss += loss.item()\n",
        "            # Calculating accuracy\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "            non_pad_elements = (target != 0).nonzero(as_tuple=True)[0]\n",
        "            val_correct = preds[non_pad_elements] == target[non_pad_elements]\n",
        "            validation_epoch_accuracy += val_correct.sum().item() / len(non_pad_elements)\n",
        "\n",
        "    validation_epoch_accuracy = validation_epoch_accuracy / len(valid_itersn)\n",
        "    validation_epoch_loss = validation_epoch_loss / len(valid_itersn)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} |Train Loss: {training_epoch_loss:.3f} | Train accuracy: {training_epoch_accuracy:.3f}|Val. Loss: {validation_epoch_loss:.3f} | Val accuracy: {validation_epoch_accuracy:.3f}')\n",
        "    wandb.log({\"train_loss\":training_epoch_loss,\"train_accuracy\": training_epoch_accuracy,\"val_loss\":validation_epoch_loss,\"val_accuracy\":validation_epoch_accuracy},)\n",
        "\n",
        "    # Emptying the cache after one complete run\n",
        "    if epoch==N_EPOCHS-1:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "#RUNNING THE SWEEP\n",
        "wandb.agent(sweep_id, function=sweep_train, count=120)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q_mn5Fzk8j9r",
        "outputId": "abbca1d3-9e67-415b-e93f-dea0a5d5df9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: sf8drmdd\n",
            "Sweep URL: https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nn3raelg with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_dimension: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dimension: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_dimension: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33med23d015\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\ASL 5\\wandb\\run-20240514_223926-nn3raelg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/nn3raelg' target=\"_blank\">grateful-sweep-1</a></strong> to <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/nn3raelg' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/runs/nn3raelg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 |Train Loss: 0.722 | Train accuracy: 0.406|Val. Loss: 0.441 | Val accuracy: 0.584\n",
            "Epoch: 02 |Train Loss: 0.348 | Train accuracy: 0.688|Val. Loss: 0.391 | Val accuracy: 0.637\n",
            "Epoch: 03 |Train Loss: 0.296 | Train accuracy: 0.734|Val. Loss: 0.393 | Val accuracy: 0.647\n",
            "Epoch: 04 |Train Loss: 0.282 | Train accuracy: 0.747|Val. Loss: 0.402 | Val accuracy: 0.644\n",
            "Epoch: 05 |Train Loss: 0.279 | Train accuracy: 0.751|Val. Loss: 0.392 | Val accuracy: 0.652\n",
            "Epoch: 06 |Train Loss: 0.271 | Train accuracy: 0.757|Val. Loss: 0.405 | Val accuracy: 0.650\n",
            "Epoch: 07 |Train Loss: 0.270 | Train accuracy: 0.760|Val. Loss: 0.401 | Val accuracy: 0.656\n",
            "Epoch: 08 |Train Loss: 0.272 | Train accuracy: 0.757|Val. Loss: 0.407 | Val accuracy: 0.647\n",
            "Epoch: 09 |Train Loss: 0.271 | Train accuracy: 0.759|Val. Loss: 0.410 | Val accuracy: 0.653\n",
            "Epoch: 10 |Train Loss: 0.275 | Train accuracy: 0.756|Val. Loss: 0.415 | Val accuracy: 0.647\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_accuracy</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.75632</td></tr><tr><td>train_loss</td><td>0.27464</td></tr><tr><td>val_accuracy</td><td>0.64717</td></tr><tr><td>val_loss</td><td>0.41456</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">grateful-sweep-1</strong> at: <a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/nn3raelg' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/runs/nn3raelg</a><br/> View project at: <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240514_223926-nn3raelg\\logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1n188z2b with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_dimension: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dimension: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_dimension: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\ASL 5\\wandb\\run-20240514_232627-1n188z2b</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/1n188z2b' target=\"_blank\">zany-sweep-2</a></strong> to <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/1n188z2b' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/runs/1n188z2b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 |Train Loss: 0.787 | Train accuracy: 0.357|Val. Loss: 0.461 | Val accuracy: 0.566\n",
            "Epoch: 02 |Train Loss: 0.323 | Train accuracy: 0.711|Val. Loss: 0.416 | Val accuracy: 0.633\n",
            "Epoch: 03 |Train Loss: 0.269 | Train accuracy: 0.759|Val. Loss: 0.398 | Val accuracy: 0.654\n",
            "Epoch: 04 |Train Loss: 0.243 | Train accuracy: 0.781|Val. Loss: 0.382 | Val accuracy: 0.666\n",
            "Epoch: 05 |Train Loss: 0.226 | Train accuracy: 0.797|Val. Loss: 0.374 | Val accuracy: 0.676\n",
            "Epoch: 06 |Train Loss: 0.215 | Train accuracy: 0.807|Val. Loss: 0.384 | Val accuracy: 0.682\n",
            "Epoch: 07 |Train Loss: 0.205 | Train accuracy: 0.817|Val. Loss: 0.368 | Val accuracy: 0.683\n",
            "Epoch: 08 |Train Loss: 0.200 | Train accuracy: 0.820|Val. Loss: 0.370 | Val accuracy: 0.688\n",
            "Epoch: 09 |Train Loss: 0.192 | Train accuracy: 0.828|Val. Loss: 0.394 | Val accuracy: 0.681\n",
            "Epoch: 10 |Train Loss: 0.189 | Train accuracy: 0.831|Val. Loss: 0.377 | Val accuracy: 0.686\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_accuracy</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.8313</td></tr><tr><td>train_loss</td><td>0.18904</td></tr><tr><td>val_accuracy</td><td>0.68615</td></tr><tr><td>val_loss</td><td>0.37686</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zany-sweep-2</strong> at: <a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/1n188z2b' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/runs/1n188z2b</a><br/> View project at: <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240514_232627-1n188z2b\\logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gw3apqd6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_dimension: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dimension: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_dimension: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\ASL 5\\wandb\\run-20240515_000945-gw3apqd6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/gw3apqd6' target=\"_blank\">flowing-sweep-3</a></strong> to <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/gw3apqd6' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/runs/gw3apqd6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 |Train Loss: 0.658 | Train accuracy: 0.460|Val. Loss: 0.423 | Val accuracy: 0.611\n",
            "Epoch: 02 |Train Loss: 0.286 | Train accuracy: 0.743|Val. Loss: 0.392 | Val accuracy: 0.656\n",
            "Epoch: 03 |Train Loss: 0.243 | Train accuracy: 0.783|Val. Loss: 0.378 | Val accuracy: 0.673\n",
            "Epoch: 04 |Train Loss: 0.220 | Train accuracy: 0.803|Val. Loss: 0.377 | Val accuracy: 0.681\n",
            "Epoch: 05 |Train Loss: 0.207 | Train accuracy: 0.815|Val. Loss: 0.367 | Val accuracy: 0.695\n",
            "Epoch: 06 |Train Loss: 0.195 | Train accuracy: 0.826|Val. Loss: 0.372 | Val accuracy: 0.694\n",
            "Epoch: 07 |Train Loss: 0.189 | Train accuracy: 0.831|Val. Loss: 0.371 | Val accuracy: 0.694\n",
            "Epoch: 08 |Train Loss: 0.181 | Train accuracy: 0.838|Val. Loss: 0.375 | Val accuracy: 0.698\n",
            "Epoch: 09 |Train Loss: 0.177 | Train accuracy: 0.842|Val. Loss: 0.375 | Val accuracy: 0.700\n",
            "Epoch: 10 |Train Loss: 0.176 | Train accuracy: 0.843|Val. Loss: 0.367 | Val accuracy: 0.703\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_accuracy</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.84261</td></tr><tr><td>train_loss</td><td>0.17626</td></tr><tr><td>val_accuracy</td><td>0.70346</td></tr><tr><td>val_loss</td><td>0.36707</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">flowing-sweep-3</strong> at: <a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/gw3apqd6' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/runs/gw3apqd6</a><br/> View project at: <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240515_000945-gw3apqd6\\logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tvfo0tc6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_dimension: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dimension: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_dimension: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\ASL 5\\wandb\\run-20240515_005554-tvfo0tc6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/tvfo0tc6' target=\"_blank\">winter-sweep-4</a></strong> to <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/tvfo0tc6' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/runs/tvfo0tc6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 |Train Loss: 0.713 | Train accuracy: 0.417|Val. Loss: 0.438 | Val accuracy: 0.600\n",
            "Epoch: 02 |Train Loss: 0.301 | Train accuracy: 0.729|Val. Loss: 0.402 | Val accuracy: 0.641\n",
            "Epoch: 03 |Train Loss: 0.254 | Train accuracy: 0.771|Val. Loss: 0.393 | Val accuracy: 0.657\n",
            "Epoch: 04 |Train Loss: 0.233 | Train accuracy: 0.790|Val. Loss: 0.368 | Val accuracy: 0.672\n",
            "Epoch: 05 |Train Loss: 0.218 | Train accuracy: 0.804|Val. Loss: 0.367 | Val accuracy: 0.679\n",
            "Epoch: 06 |Train Loss: 0.209 | Train accuracy: 0.814|Val. Loss: 0.380 | Val accuracy: 0.680\n",
            "Epoch: 07 |Train Loss: 0.202 | Train accuracy: 0.820|Val. Loss: 0.367 | Val accuracy: 0.689\n",
            "Epoch: 08 |Train Loss: 0.194 | Train accuracy: 0.826|Val. Loss: 0.376 | Val accuracy: 0.686\n",
            "Epoch: 09 |Train Loss: 0.188 | Train accuracy: 0.831|Val. Loss: 0.389 | Val accuracy: 0.684\n",
            "Epoch: 10 |Train Loss: 0.183 | Train accuracy: 0.836|Val. Loss: 0.373 | Val accuracy: 0.694\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_accuracy</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.83553</td></tr><tr><td>train_loss</td><td>0.1833</td></tr><tr><td>val_accuracy</td><td>0.69446</td></tr><tr><td>val_loss</td><td>0.37348</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">winter-sweep-4</strong> at: <a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/tvfo0tc6' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/runs/tvfo0tc6</a><br/> View project at: <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240515_005554-tvfo0tc6\\logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vttv67s5 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_dimension: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dimension: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_dimension: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\ASL 5\\wandb\\run-20240515_013804-vttv67s5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/vttv67s5' target=\"_blank\">divine-sweep-5</a></strong> to <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/vttv67s5' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/runs/vttv67s5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 |Train Loss: 0.771 | Train accuracy: 0.368|Val. Loss: 0.441 | Val accuracy: 0.576\n",
            "Epoch: 02 |Train Loss: 0.322 | Train accuracy: 0.710|Val. Loss: 0.391 | Val accuracy: 0.634\n",
            "Epoch: 03 |Train Loss: 0.267 | Train accuracy: 0.758|Val. Loss: 0.396 | Val accuracy: 0.659\n",
            "Epoch: 04 |Train Loss: 0.248 | Train accuracy: 0.776|Val. Loss: 0.382 | Val accuracy: 0.666\n",
            "Epoch: 05 |Train Loss: 0.233 | Train accuracy: 0.789|Val. Loss: 0.363 | Val accuracy: 0.678\n",
            "Epoch: 06 |Train Loss: 0.223 | Train accuracy: 0.798|Val. Loss: 0.365 | Val accuracy: 0.682\n",
            "Epoch: 07 |Train Loss: 0.214 | Train accuracy: 0.807|Val. Loss: 0.374 | Val accuracy: 0.687\n",
            "Epoch: 08 |Train Loss: 0.209 | Train accuracy: 0.812|Val. Loss: 0.362 | Val accuracy: 0.687\n",
            "Epoch: 09 |Train Loss: 0.202 | Train accuracy: 0.818|Val. Loss: 0.356 | Val accuracy: 0.691\n",
            "Epoch: 10 |Train Loss: 0.198 | Train accuracy: 0.822|Val. Loss: 0.371 | Val accuracy: 0.693\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_accuracy</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.82182</td></tr><tr><td>train_loss</td><td>0.19806</td></tr><tr><td>val_accuracy</td><td>0.69299</td></tr><tr><td>val_loss</td><td>0.37051</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">divine-sweep-5</strong> at: <a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/vttv67s5' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/runs/vttv67s5</a><br/> View project at: <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240515_013804-vttv67s5\\logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pxoik5d9 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_dimension: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dimension: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_dimension: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\ASL 5\\wandb\\run-20240515_021953-pxoik5d9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/pxoik5d9' target=\"_blank\">deft-sweep-6</a></strong> to <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed23d015/DL_Assignment_3' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/sweeps/sf8drmdd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed23d015/DL_Assignment_3/runs/pxoik5d9' target=\"_blank\">https://wandb.ai/ed23d015/DL_Assignment_3/runs/pxoik5d9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 |Train Loss: 0.821 | Train accuracy: 0.324|Val. Loss: 0.555 | Val accuracy: 0.452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla Sequence to Sequence Model"
      ],
      "metadata": {
        "id": "4es7SQFZMaUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model_bkl: nn.Module, iterator: torch.utils.data.DataLoader, optimizer: optim.Optimizer, criterion: nn.Module, clip: float):\n",
        "\n",
        "    model_bkl_bkl.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    epoch_word_accuracy = 0\n",
        "    total_charecters = 0\n",
        "    correct_charecters = 0\n",
        "    total_words = 0\n",
        "    correct_words = 0\n",
        "\n",
        "\n",
        "    for _, (source, target) in enumerate(iterator):\n",
        "        source, target = source.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model_bkl(source, target)\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        target = target.permute(1,0)\n",
        "        target = torch.reshape(target[1:], (-1,))\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # calculate character level accuracy\n",
        "        preds = torch.argmax(output, dim=1)\n",
        "        non_pad_elements = (target != 0).nonzero(as_tuple=True)[0]\n",
        "        correct = preds[non_pad_elements] == target[non_pad_elements]\n",
        "        epoch_accuracy += correct.sum().item() / len(non_pad_elements)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model_bkl.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model_bkl: nn.Module,\n",
        "             iterator: torch.utils.data.DataLoader,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    model_bkl.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    epoch_word_accuracy = 0\n",
        "    total_charecters = 0\n",
        "    correct_charecters = 0\n",
        "    total_words = 0\n",
        "    correct_words = 0\n",
        "    solution=[]\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, (source, target) in enumerate(iterator):\n",
        "            source, target = source.to(device), target.to(device)\n",
        "\n",
        "            output = model_bkl(source, target, 0) #turn off teacher forcing\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            target = target.permute(1,0)\n",
        "            target = torch.reshape(target[1:], (-1,))\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # calculate accuracy\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "            non_pad_elements = (target != 0).nonzero(as_tuple=True)[0]\n",
        "            correct = preds[non_pad_elements] == target[non_pad_elements]\n",
        "            epoch_accuracy += correct.sum().item() / len(non_pad_elements)\n",
        "\n",
        "            b=np.zeros((16,29))\n",
        "            for i in range(16):\n",
        "              for j in range (29):\n",
        "                 b[i][j]=preds[16*j+i]\n",
        "\n",
        "            for i in range(16):\n",
        "              solution.append(reverse_tokenize(b[i]))\n",
        "\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator), solution\n",
        "\n",
        "\n",
        "def epoch_time(start_time: int,\n",
        "               end_time: int):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "CLIP = 1\n",
        "num_epoch = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss,train_acc = train(model_bkl_bkl, train_itersn, optimizer, criterion, CLIP)\n",
        "    valid_loss,valid_accuracy,solution_valid = evaluate(model_bkl_bkl, valid_itersn, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_in_mins, epoch_in_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_in_mins}m {epoch_in_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train accuracy: {train_acc:.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} | Val accuracy: {valid_accuracy:.3f}')\n",
        "    print(solution_valid[0])\n",
        "\n",
        "test_loss,test_accuracy,solution_test_att = evaluate(model_bkl, train_itersn, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test accuracy: {test_accuracy:.3f} |')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "Y1RliBYNjSRm",
        "outputId": "646fa554-1bf6-40a4-dda7-1336eebf46da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model_bkl: nn\u001b[38;5;241m.\u001b[39mModule, iterator: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader, optimizer: optim\u001b[38;5;241m.\u001b[39mOptimizer, criterion: nn\u001b[38;5;241m.\u001b[39mModule, clip: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[0;32m      9\u001b[0m     model_bkl_bkl\u001b[38;5;241m.\u001b[39mtrain()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla implimentation on Wandb"
      ],
      "metadata": {
        "id": "8cBGRudnY5ER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes', #grid, random,bayes\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'lr': {\n",
        "            'values': [0.001,0.0001]\n",
        "        },\n",
        "        'layer_size': {\n",
        "            'values': [1,2,3,4]\n",
        "        },\n",
        "        'cell_type':{\n",
        "            'values':['rnn','lstm','gru']\n",
        "        },\n",
        "        'dropout':{\n",
        "            'values':[0,0.2,0.4,0.6]\n",
        "        },\n",
        "        'hidden_layers':{\n",
        "            'values':[64,128,256]\n",
        "        },\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity='shreyashgadgil007', project=\"CS6910-Assignment3\")\n",
        "\n",
        "def sweep_train():\n",
        "  # Default values for hyper-parameters we're going to sweep over\n",
        "  config_defaults = {\n",
        "      'lr':0.0001,\n",
        "      'layer_size':4,\n",
        "      'cell_type':'lstm',\n",
        "      'dropout':0.4,\n",
        "      'hidden_layers':128,\n",
        "  }\n",
        "\n",
        "  # Initialize a new wandb run\n",
        "  wandb.init(project='CS6910-Assignment3', entity='shreyashgadgil007',config=config_defaults)\n",
        "  wandb.run.name = 'cell:'+ str(wandb.config.cell_type)+' ;lr:'+str(wandb.config.lr)+ ' ;layer_size:'+str(wandb.config.layer_size)+ ' ;dropout:'+str(wandb.config.dropout)+' ;hidden:'+str(wandb.config.hidden_layers)\n",
        "  config = wandb.config\n",
        "  lr = config.lr\n",
        "  layer_size = config.layer_size\n",
        "  cell_type = config.cell_type\n",
        "  hidden_layers = config.hidden_layers\n",
        "  dropout = config.dropout\n",
        "  # Model training here\n",
        "\n",
        "  input_size_encoder = len(eng_token_map)\n",
        "  output_size=input_size_decoder=len(mar_token_map)\n",
        "\n",
        "  encoder_embedding_size=29\n",
        "  decoder_embedding_size=67\n",
        "  encoder_net=Encoder(input_size_encoder,encoder_embedding_size,hidden_layers,layer_size,cell_type,p=dropout).to(device)\n",
        "  decoder_net=Decoder(input_size_decoder,decoder_embedding_size,hidden_layers,output_size,layer_size,cell_type,p=dropout).to(device)\n",
        "  model=Seq2Seq(encoder_net,decoder_net,cell_type).to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  import math\n",
        "  import time\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(),lr=lr)\n",
        "  model.train()\n",
        "\n",
        "  training_epoch_loss = 0\n",
        "  training_epoch_accuracy = 0\n",
        "  validation_epoch_loss = 0\n",
        "  validation_epoch_accuracy = 0\n",
        "\n",
        "  N_EPOCHS = 10\n",
        "  CLIP = 1\n",
        "\n",
        "  for epoch in range(N_EPOCHS):\n",
        "\n",
        "  #TRAINING BLOCK\n",
        "    model.train()\n",
        "    for _, (source, target) in enumerate(train_iter):\n",
        "        source, target = source.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(source, target)\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        target = target.permute(1,0)\n",
        "        target = torch.reshape(target[1:], (-1,))\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # calculate accuracy\n",
        "        preds = torch.argmax(output, dim=1)\n",
        "        non_pad_elements = (target != 0).nonzero(as_tuple=True)[0]\n",
        "        train_correct = preds[non_pad_elements] == target[non_pad_elements]\n",
        "        training_epoch_accuracy += train_correct.sum().item() / len(non_pad_elements)\n",
        "        loss.backward()\n",
        "\n",
        "        #READ IF WE CAN PASS CLIP AS A HYPERPARAMETER\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        training_epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "    training_epoch_loss = training_epoch_loss / len(train_iter)\n",
        "    training_epoch_accuracy = training_epoch_accuracy / len(train_iter)\n",
        "\n",
        "\n",
        "    #EVALUATION MODE\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, (source, target) in enumerate(valid_iter):\n",
        "            source, target = source.to(device), target.to(device)\n",
        "\n",
        "            output = model(source, target, 0) #turn off teacher forcing\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            target = target.permute(1,0)\n",
        "            target = torch.reshape(target[1:], (-1,))\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            validation_epoch_loss += loss.item()\n",
        "            # calculate accuracy\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "            non_pad_elements = (target != 0).nonzero(as_tuple=True)[0]\n",
        "            val_correct = preds[non_pad_elements] == target[non_pad_elements]\n",
        "            validation_epoch_accuracy += val_correct.sum().item() / len(non_pad_elements)\n",
        "\n",
        "    validation_epoch_loss = validation_epoch_loss / len(valid_iter)\n",
        "    validation_epoch_accuracy = validation_epoch_accuracy / len(valid_iter)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} |Train Loss: {training_epoch_loss:.3f} | Train accuracy: {training_epoch_accuracy:.3f}|Val. Loss: {validation_epoch_loss:.3f} | Val accuracy: {validation_epoch_accuracy:.3f}')\n",
        "    wandb.log({\"train_loss\":training_epoch_loss,\"train_accuracy\": training_epoch_accuracy,\"val_loss\":validation_epoch_loss,\"val_accuracy\":validation_epoch_accuracy},)\n",
        "    #emptying the cache after one complete run\n",
        "    if epoch==N_EPOCHS-1:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "#RUNNING THE SWEEP\n",
        "wandb.agent(sweep_id, function=sweep_train, count=120)\n"
      ],
      "metadata": {
        "id": "7XilzV75Y2XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSV files and Word level accuracy"
      ],
      "metadata": {
        "id": "u6_EGlcOZHok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing end of sentence token\n",
        "solution_test_att = [word.replace('.', '') for word in solution_test_att]\n",
        "solution_test = [word.replace('.', '') for word in solution_test]\n",
        "def calculate_accuracy(list1, list2):\n",
        "    total_words = len(list1)\n",
        "    correct_words = 0\n",
        "\n",
        "    for word1, word2 in zip(list1, list2):\n",
        "        if word1 == word2:\n",
        "            correct_words += 1\n",
        "\n",
        "    accuracy = correct_words / total_words * 100\n",
        "    return accuracy\n",
        "\n",
        "vanilla_seq2seq_accuracy=calculate_accuracy(test_ma,solution_test)\n",
        "attention_seq2seq_accuracy=calculate_accuracy(test_ma,solution_test_att)\n",
        "print('vanilla_seq2seq_Acc.:',vanilla_seq2seq_accuracy)\n",
        "print('attention_seq2seq_Acc.:',attention_seq2seq_accuracy)"
      ],
      "metadata": {
        "id": "PTv4z41cZFno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Predictions_attention.csv'\n",
        "with open(file_path, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the column headers\n",
        "    writer.writerow(['ENGLISH', 'HINDI_translation', 'Attention_seq2seq_pred'])\n",
        "\n",
        "    # Write the data rows\n",
        "    for row in zip(test_en, test_ma, solution_test_att):\n",
        "        writer.writerow(row)"
      ],
      "metadata": {
        "id": "TS3P4TLMawgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "file_path = '/content/drive/MyDrive/Predictions_vanilla.csv'\n",
        "with open(file_path, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the column headers\n",
        "    writer.writerow(['ENGLISH', 'HINDI_translation', 'Vanilla_seq2seq_pred'])\n",
        "\n",
        "    # Write the data rows\n",
        "    for row in zip(test_en, test_ma, solution_test):\n",
        "        writer.writerow(row)"
      ],
      "metadata": {
        "id": "rrK51JlnayWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pay \"Attension\" Here!! remove the code below"
      ],
      "metadata": {
        "id": "Hhf_JITdNWi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size: int, embedding_dim: int, hidden_size: int, num_layers=1, cell_type='LSTM', bidirectional=False, dropout=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        if cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "        elif cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "        elif cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return outputs, (hidden, cell)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, embedding_dim, hidden_size, num_layers=1, cell_type='LSTM', dropout=0, beam_search=False, beam_size=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.beam_search = beam_search\n",
        "        self.beam_size = beam_size\n",
        "\n",
        "        if cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, dropout=dropout)\n",
        "        elif cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers=num_layers, dropout=dropout)\n",
        "        elif cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers=num_layers, dropout=dropout)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self,\n",
        "                decoder_hidden: Tensor,\n",
        "                encoder_outputs: Tensor) -> Tensor:\n",
        "\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        energy = torch.tanh(self.attn(torch.cat((\n",
        "            repeated_decoder_hidden,\n",
        "            encoder_outputs),\n",
        "            dim = 2)))\n",
        "\n",
        "        attention = torch.sum(energy, dim=2)\n",
        "\n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "\n",
        "    '''\n",
        "    def forward(self, x, hidden, cell=None):\n",
        "\n",
        "        if self.beam_search:\n",
        "            return self.beam_search_decoder(x, hidden)\n",
        "        else:\n",
        "            return self.teacher_forcing(x, hidden, cell)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def teacher_forcing(self, x, hidden, cell):\n",
        "        x = x.unsqueeze(0)\n",
        "        print(\"x is here: \",x)\n",
        "        print(\"hidden: \", hidden.shape)\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.fc(output.squeeze(0))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "    def beam_search_decoder(self, x, hidden):  # Beter then greedy search\n",
        "        #print(\"x_shape: \", x.shape)\n",
        "        batch_size = x.shape[0]\n",
        "        target_len = 1  # Beam search for single token at a time\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, self.output_size).to(x.device)\n",
        "        sequences = [[[0], 0.0, hidden]]\n",
        "\n",
        "        for _ in range(target_len):\n",
        "            all_candidates = []\n",
        "            for i in range(len(sequences)):\n",
        "                #print(\"sequences: \", sequences)\n",
        "                seq, score, hidden = sequences[i]\n",
        "                print(\"seq: \", seq)\n",
        "                print(\"score: \", score)\n",
        "                print(\"hidden: \", hidden.shape)\n",
        "                print(\"x: \",x)\n",
        "                x = torch.tensor([seq[-1]]).unsqueeze(0).unsqueeze(0).to(x.device)  # Fix here\n",
        "                #x = torch.tensor([seq[-1]]).unsqueeze(0).to(x.device)\n",
        "                print(\"x after: \",x)\n",
        "                embedded = self.embedding(x)\n",
        "                output, (hidden, cell) = self.rnn(embedded, hidden)\n",
        "                log_prob = nn.functional.log_softmax(output, dim=1)\n",
        "                topk_log_prob, topk_indices = torch.topk(log_prob, self.beam_size)\n",
        "                for j in range(self.beam_size):\n",
        "                    candidate_seq = seq + [topk_indices[0][0][j].item()]\n",
        "                    candidate_score = score + topk_log_prob[0][0][j].item()\n",
        "                    all_candidates.append([candidate_seq, candidate_score, hidden])\n",
        "            ordered = sorted(all_candidates, key=lambda tup:tup[1], reverse=True)\n",
        "            sequences = ordered[:self.beam_size]\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            outputs[0][i][sequences[i][0][-1]] = 1.0\n",
        "\n",
        "        return outputs, sequences[0][2]\n",
        "        '''\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = self.decoder.output_size\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(source.device)\n",
        "\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(source)\n",
        "\n",
        "        x = target[0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            teacher_force = np.random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            x = target[t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Example usage:\n",
        "input_size = len(eng_token_map)\n",
        "output_size = len(hin_token_map)\n",
        "encoder_embedding_dim = 64\n",
        "decoder_embedding_dim = 64\n",
        "hidden_size = 128\n",
        "num_encoder_layers = 2\n",
        "num_decoder_layers = 2\n",
        "encoder_bidirectional = True\n",
        "encoder_dropout = 0.1\n",
        "decoder_dropout = 0.1\n",
        "beam_search = True\n",
        "beam_size = 3\n",
        "\n",
        "encoder = Encoder(input_size, encoder_embedding_dim, hidden_size, num_layers=num_encoder_layers, cell_type='LSTM', bidirectional=encoder_bidirectional, dropout=encoder_dropout)\n",
        "decoder = Decoder(output_size, decoder_embedding_dim, hidden_size, num_layers=num_decoder_layers, cell_type='LSTM', dropout=decoder_dropout, beam_search=beam_search, beam_size=beam_size)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "\n",
        "# Example input and output tensors\n",
        "source = torch.randint(0, input_size, (10, 32))  # (seq_len, batch_size)\n",
        "target = torch.randint(0, output_size, (8, 32))  # (seq_len, batch_size)\n",
        "\n",
        "output = model(source, target)\n",
        "print(output.shape)  # Output shape: (target_seq_len, batch_size, output_vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "xs4TVE_oho5d",
        "outputId": "5073aa37-36fc-466f-8808-5f738de1bb36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x is here:  tensor([[11, 23, 29, 26, 28, 28, 18, 53, 50,  9, 26, 16,  1, 25, 45, 28, 23,  0,\n",
            "         54,  9, 18, 23, 58, 50,  9, 55, 34, 44, 10, 35, 63,  6]])\n",
            "hidden:  torch.Size([4, 32, 128])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected hidden[0] size (2, 32, 128), got [4, 32, 128]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-66eb85f08958>\u001b[0m in \u001b[0;36m<cell line: 148>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (seq_len, batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Output shape: (target_seq_len, batch_size, output_vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-66eb85f08958>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, target, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mteacher_force\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-66eb85f08958>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden, cell)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteacher_forcing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         '''\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteacher_forcing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-66eb85f08958>\u001b[0m in \u001b[0;36mteacher_forcing\u001b[0;34m(self, x, hidden, cell)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hidden: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;31m# Each batch of the hidden state should match the input sequence that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;31m# the user believes he/she is passing in.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m                 \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    788\u001b[0m                            ):\n\u001b[1;32m    789\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0m\u001b[1;32m    791\u001b[0m                                'Expected hidden[0] size {}, got {}')\n\u001b[1;32m    792\u001b[0m         self.check_hidden_size(hidden[1], self.get_expected_cell_size(input, batch_sizes),\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    257\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_weights_have_changed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 32, 128), got [4, 32, 128]"
          ]
        }
      ]
    }
  ]
}