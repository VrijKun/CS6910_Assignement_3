{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNj6mkIum8D6vQ9OjM0l3St",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VrijKun/CS6910_Assignement_3/blob/main/DL_Assignement_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_47MUGx8NNaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00dbe0e4-5bc8-44c7-fa22-211adc453924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.2.3-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from pytorch-lightning) (2.2.2)\n",
            "Collecting tqdm>=4.57.0 (from pytorch-lightning)\n",
            "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
            "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
            "     ---------------------------- ----------- 41.0/57.6 kB 2.0 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 41.0/57.6 kB 2.0 MB/s eta 0:00:01\n",
            "     -------------------------------------- 57.6/57.6 kB 434.4 kB/s eta 0:00:00\n",
            "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from pytorch-lightning) (6.0.1)\n",
            "Collecting fsspec>=2022.5.0 (from fsspec[http]>=2022.5.0->pytorch-lightning)\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from pytorch-lightning) (4.9.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2022.5.0->pytorch-lightning)\n",
            "  Downloading aiohttp-3.9.5-cp38-cp38-win_amd64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: setuptools in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (68.2.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from torch>=1.13.0->pytorch-lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from torch>=1.13.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from torch>=1.13.0->pytorch-lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from torch>=1.13.0->pytorch-lightning) (3.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.6)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.1.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
            "  Downloading frozenlist-1.4.1-cp38-cp38-win_amd64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
            "  Downloading multidict-6.0.5-cp38-cp38-win_amd64.whl.metadata (4.3 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
            "  Downloading yarl-1.9.4-cp38-cp38-win_amd64.whl.metadata (32 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in c:\\users\\asl 5\\.conda\\envs\\cuda121_pytorch222\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.4)\n",
            "Downloading pytorch_lightning-2.2.3-py3-none-any.whl (802 kB)\n",
            "   ---------------------------------------- 0.0/802.2 kB ? eta -:--:--\n",
            "   - ------------------------------------- 41.0/802.2 kB 991.0 kB/s eta 0:00:01\n",
            "   ---- ----------------------------------- 92.2/802.2 kB 1.1 MB/s eta 0:00:01\n",
            "   ------- -------------------------------- 143.4/802.2 kB 1.1 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 204.8/802.2 kB 1.1 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 256.0/802.2 kB 1.2 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 368.6/802.2 kB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 450.6/802.2 kB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 573.4/802.2 kB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 686.1/802.2 kB 1.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  798.7/802.2 kB 1.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 802.2/802.2 kB 1.4 MB/s eta 0:00:00\n",
            "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "   ---------------------------------------- 0.0/172.0 kB ? eta -:--:--\n",
            "   ------------------- -------------------- 81.9/172.0 kB 1.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 172.0/172.0 kB 1.5 MB/s eta 0:00:00\n",
            "Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "   ---------------------------------------- 0.0/841.5 kB ? eta -:--:--\n",
            "   - -------------------------------------- 41.0/841.5 kB 2.0 MB/s eta 0:00:01\n",
            "   ----- ---------------------------------- 112.6/841.5 kB 1.7 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 194.6/841.5 kB 1.7 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 286.7/841.5 kB 1.6 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 286.7/841.5 kB 1.6 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 337.9/841.5 kB 1.2 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 409.6/841.5 kB 1.3 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 491.5/841.5 kB 1.3 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 532.5/841.5 kB 1.3 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 573.4/841.5 kB 1.3 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 655.4/841.5 kB 1.3 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 655.4/841.5 kB 1.3 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 696.3/841.5 kB 1.1 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 819.2/841.5 kB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 841.5/841.5 kB 1.2 MB/s eta 0:00:00\n",
            "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
            "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
            "   --------------- ------------------------ 30.7/78.3 kB ? eta -:--:--\n",
            "   ---------------------------------------- 78.3/78.3 kB 872.5 kB/s eta 0:00:00\n",
            "Downloading aiohttp-3.9.5-cp38-cp38-win_amd64.whl (373 kB)\n",
            "   ---------------------------------------- 0.0/373.1 kB ? eta -:--:--\n",
            "   ---------- ----------------------------- 102.4/373.1 kB 2.9 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 163.8/373.1 kB 1.6 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 307.2/373.1 kB 1.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 358.4/373.1 kB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 373.1/373.1 kB 1.4 MB/s eta 0:00:00\n",
            "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading frozenlist-1.4.1-cp38-cp38-win_amd64.whl (50 kB)\n",
            "   ---------------------------------------- 0.0/50.8 kB ? eta -:--:--\n",
            "   ---------------------------------------- 50.8/50.8 kB 1.3 MB/s eta 0:00:00\n",
            "Downloading multidict-6.0.5-cp38-cp38-win_amd64.whl (28 kB)\n",
            "Downloading yarl-1.9.4-cp38-cp38-win_amd64.whl (77 kB)\n",
            "   ---------------------------------------- 0.0/77.1 kB ? eta -:--:--\n",
            "   --------------------- ------------------ 41.0/77.1 kB 991.0 kB/s eta 0:00:01\n",
            "   ---------------------------------------- 77.1/77.1 kB 856.6 kB/s eta 0:00:00\n",
            "Installing collected packages: tqdm, multidict, lightning-utilities, fsspec, frozenlist, async-timeout, yarl, aiosignal, torchmetrics, aiohttp, pytorch-lightning\n",
            "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 fsspec-2024.3.1 lightning-utilities-0.11.2 multidict-6.0.5 pytorch-lightning-2.2.3 torchmetrics-1.3.2 tqdm-4.66.2 yarl-1.9.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yes | wget \"https://drive.google.com/drive/folders/1tmm5HT2Zwj-4vDZzRFc4Av4KoWHYowoG?usp=sharing\""
      ],
      "metadata": {
        "id": "DNTkPBjP9dlM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb1663f-bf62-421e-97db-9d5ee9606f0b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-30 15:44:53--  https://drive.google.com/drive/folders/1tmm5HT2Zwj-4vDZzRFc4Av4KoWHYowoG?usp=sharing\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.219.138, 172.217.219.100, 172.217.219.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.219.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘1tmm5HT2Zwj-4vDZzRFc4Av4KoWHYowoG?usp=sharing’\n",
            "\n",
            "1tmm5HT2Zwj-4vDZzRF     [  <=>               ] 401.13K  1.36MB/s    in 0.3s    \n",
            "\n",
            "2024-04-30 15:44:54 (1.36 MB/s) - ‘1tmm5HT2Zwj-4vDZzRFc4Av4KoWHYowoG?usp=sharing’ saved [410761]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vIU8i0W7h5V",
        "outputId": "af31eccf-7340-4fcb-abd9-b0c24cb9ed13"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pds\n",
        "\n",
        "# function for loading data\n",
        "def dt_ld(path):\n",
        "  with open(path) as fil :\n",
        "    dt = pds.read_csv(fil,sep=',',header=None,names=[\"eng\",\"hin\",\"\"],skip_blank_lines=True,index_col=None)\n",
        "  word_data = dt[dt['eng'].notna()]\n",
        "  word_data = dt[dt['hin'].notna()]\n",
        "  word_data = dt[['eng', 'hin']]\n",
        "  return word_data\n",
        "\n",
        "train_data = dt_ld(\"/content/drive/MyDrive/aksharantar_sampled/hin/hin_train.csv\")\n",
        "valid_data = dt_ld(\"/content/drive/MyDrive/aksharantar_sampled/hin/hin_valid.csv\")\n",
        "test_data = dt_ld(\"/content/drive/MyDrive/aksharantar_sampled/hin/hin_test.csv\")\n",
        "\n",
        "# Saving the data in CSV's again in list form\n",
        "test_hin = list(test_data['hin'])\n",
        "test_eng = list(test_data['eng'])\n",
        "\n",
        "# Some visualization of data\n",
        "print(len(train_data))\n",
        "print(len(valid_data))\n",
        "print(len(test_data))\n",
        "#print(\"test_hindi: \", test_hin)\n",
        "#print(\"test_english: \", test_eng)\n",
        "\n"
      ],
      "metadata": {
        "id": "1d_5GXyE-dLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62349e5f-bb4a-4c25-b796-b755234fb5cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51200\n",
            "4096\n",
            "4096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating tokenizied dataset here\n",
        "\n",
        "# Define a function to generate unique tokens for Hindi and English words from a dataset\n",
        "def uni_tok(dataset):\n",
        "  # Extract Hindi and English sentences from the dataset\n",
        "  hin = dataset['hin'].values\n",
        "  eng = dataset['eng'].values\n",
        "\n",
        "  # Initialize sets to store unique tokens for Hindi and English\n",
        "  hin_token = set()\n",
        "  eng_token = set()\n",
        "\n",
        "  # Iterate through pairs of Hindi and English sentences\n",
        "  for i, j in zip(eng, hin):\n",
        "    # Iterate through characters in Hindi sentence\n",
        "    for chare in j:\n",
        "      # Add each character to Hindi tokens set\n",
        "      hin_token.add(chare)\n",
        "    # Iterate through characters in English sentence\n",
        "    for chare in i:\n",
        "      # Add each character to English tokens set\n",
        "      eng_token.add(chare)\n",
        "\n",
        "  # Sort the sets and convert them to lists\n",
        "  hin_token = sorted(list(hin_token))\n",
        "  eng_token = sorted(list(eng_token))\n",
        "\n",
        "  # Return the lists of unique Hindi and English tokens\n",
        "  return hin_token, eng_token\n",
        "\n",
        "# Call the function to generate tokens for the training data\n",
        "hindi_token, english_token = uni_tok(train_data)\n",
        "\n",
        "# Print some statistics and samples of the generated tokens\n",
        "print(\"english_token : \", english_token)\n",
        "print(\"length of english words : \", len(english_token))\n",
        "print(\"hindi_token : \", hindi_token)\n",
        "print(\"length of hindi words : \", len(hindi_token))\n",
        "\n",
        "# \"Matraon\" ke liye alag se token hai mtlb ki"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOO8dojV7snM",
        "outputId": "771f362a-1d16-4fb0-9863-1b01e9f6039c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "english_token :  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "length of english words :  26\n",
            "hindi_token :  ['ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'ळ', 'व', 'श', 'ष', 'स', 'ह', '़', 'ऽ', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॅ', 'े', 'ै', 'ॉ', 'ो', 'ौ', '्']\n",
            "length of hindi words :  64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating token map here\n",
        "\n",
        "def tokenize_map(language_tokens , english_tokens):\n",
        "\n",
        "    # Create English token map with characters as keys and their index + 1 as values\n",
        "    english_token_map = dict([(ch,i+1) for i,ch in enumerate(english_tokens)])\n",
        "\n",
        "    # Create language token map with characters as keys and their index + 1 as values\n",
        "    language_token_map = dict([(ch,i+1) for i,ch in enumerate(language_tokens)])\n",
        "\n",
        "    # Create a reverse language token map with index + 1 as keys and characters as values\n",
        "    reverse_language_token_map = dict([(i+1,ch) for i,ch in enumerate(language_tokens)])\n",
        "\n",
        "    # Adding blank space to both token maps with value 0\n",
        "    language_token_map[\" \"] = 0\n",
        "    english_token_map[\" \"] = 0\n",
        "\n",
        "    # Add special tokens for beginning and end of sentence in language token map\n",
        "    language_token_map[';']=65\n",
        "    language_token_map['.']=66\n",
        "\n",
        "    # Add special tokens for beginning and end of sentence in English token map\n",
        "    english_token_map[';']=27\n",
        "    english_token_map['.']=28\n",
        "\n",
        "    # Add <unk> token for unknown characters to language token maps\n",
        "    language_token_map['<unk>']=64\n",
        "\n",
        "    # Update the reverse language token map with special tokens\n",
        "    reverse_language_token_map[64]='<unk>'\n",
        "    reverse_language_token_map[65]=';'\n",
        "    reverse_language_token_map[66]='.'\n",
        "    reverse_language_token_map[0]=''\n",
        "\n",
        "    # Return the Marathi token map, English token map, and reverse Marathi token map\n",
        "    return language_token_map, reverse_language_token_map, english_token_map\n",
        "\n",
        "# Call the function to generate token maps for hindi and english\n",
        "hin_token_map, reverse_hin_token_map, eng_token_map = tokenize_map(hindi_token, english_token)\n",
        "\n",
        "# Print the generated token maps\n",
        "print(hin_token_map)\n",
        "print(reverse_hin_token_map)\n",
        "print(eng_token_map)\n",
        "\n",
        "# Print the lengths of Marathi and English token maps\n",
        "print('marathi token map length:', len(hin_token_map))\n",
        "print('english token map length:', len(eng_token_map))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP1SUzLZZx1X",
        "outputId": "21e13889-6577-4ca9-a94e-3a2d0e8ec247"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ँ': 1, 'ं': 2, 'ः': 3, 'अ': 4, 'आ': 5, 'इ': 6, 'ई': 7, 'उ': 8, 'ऊ': 9, 'ऋ': 10, 'ए': 11, 'ऐ': 12, 'ऑ': 13, 'ओ': 14, 'औ': 15, 'क': 16, 'ख': 17, 'ग': 18, 'घ': 19, 'ङ': 20, 'च': 21, 'छ': 22, 'ज': 23, 'झ': 24, 'ञ': 25, 'ट': 26, 'ठ': 27, 'ड': 28, 'ढ': 29, 'ण': 30, 'त': 31, 'थ': 32, 'द': 33, 'ध': 34, 'न': 35, 'प': 36, 'फ': 37, 'ब': 38, 'भ': 39, 'म': 40, 'य': 41, 'र': 42, 'ल': 43, 'ळ': 44, 'व': 45, 'श': 46, 'ष': 47, 'स': 48, 'ह': 49, '़': 50, 'ऽ': 51, 'ा': 52, 'ि': 53, 'ी': 54, 'ु': 55, 'ू': 56, 'ृ': 57, 'ॅ': 58, 'े': 59, 'ै': 60, 'ॉ': 61, 'ो': 62, 'ौ': 63, '्': 64, ' ': 0, ';': 65, '.': 66, '<unk>': 64}\n",
            "{1: 'ँ', 2: 'ं', 3: 'ः', 4: 'अ', 5: 'आ', 6: 'इ', 7: 'ई', 8: 'उ', 9: 'ऊ', 10: 'ऋ', 11: 'ए', 12: 'ऐ', 13: 'ऑ', 14: 'ओ', 15: 'औ', 16: 'क', 17: 'ख', 18: 'ग', 19: 'घ', 20: 'ङ', 21: 'च', 22: 'छ', 23: 'ज', 24: 'झ', 25: 'ञ', 26: 'ट', 27: 'ठ', 28: 'ड', 29: 'ढ', 30: 'ण', 31: 'त', 32: 'थ', 33: 'द', 34: 'ध', 35: 'न', 36: 'प', 37: 'फ', 38: 'ब', 39: 'भ', 40: 'म', 41: 'य', 42: 'र', 43: 'ल', 44: 'ळ', 45: 'व', 46: 'श', 47: 'ष', 48: 'स', 49: 'ह', 50: '़', 51: 'ऽ', 52: 'ा', 53: 'ि', 54: 'ी', 55: 'ु', 56: 'ू', 57: 'ृ', 58: 'ॅ', 59: 'े', 60: 'ै', 61: 'ॉ', 62: 'ो', 63: 'ौ', 64: '<unk>', 65: ';', 66: '.', 0: ''}\n",
            "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, ' ': 0, ';': 27, '.': 28}\n",
            "marathi token map length: 68\n",
            "english token map length: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets see what is the maximun word length avaiolable in the dataset\n",
        "\n",
        "# Assigning values from the 'hin' and 'eng' columns of the test_data DataFrame to variables 'hiii' and 'ennn' respectively.\n",
        "hiii = test_data['hin'].values\n",
        "ennn = test_data['eng'].values\n",
        "\n",
        "# Adding ';' at the beginning and '.' at the end of each element in the 'hiii' and 'ennn' arrays.\n",
        "# Adding special characters like ';' and '.' the start and end of a sentence or phrase, which is necessary for subsequent processing or analysis being performed on the strings.\n",
        "hiii = ';' + hiii + '.'\n",
        "ennn = ';' + ennn + '.'\n",
        "\n",
        "#print(hiii)\n",
        "\n",
        "# Finding the maximum length of a string in the 'hiii' array.\n",
        "maximum_hin = max([len(i) for i in hiii])\n",
        "# Finding the maximum length of a string in the 'ennn' array.\n",
        "maximum_eng = max([len(i) for i in ennn])\n",
        "\n",
        "# Printing the maximum word length in Hindi.\n",
        "print(\"maximum word length in hindi is : \", maximum_hin)\n",
        "# Printing the maximum word length in English.\n",
        "print(\"maximum word length in english is : \", maximum_eng)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t1NmzzG-FEi",
        "outputId": "044f9c77-1e13-46a8-ef4e-f2c8fc0d06e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[';थरमैक्स.' ';सिखाएगा.' ';लर्न.' ... ';खातूटोला.' ';शिवास्तव.'\n",
            " ';प्रेरणापुरी.']\n",
            "maximum word length in hindi is :  22\n",
            "maximum word length in english is :  28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOw we do one hot encoding\n",
        "\n"
      ],
      "metadata": {
        "id": "Y50ML0xPARbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pay \"Attension\" Here!!"
      ],
      "metadata": {
        "id": "Hhf_JITdNWi-"
      }
    }
  ]
}